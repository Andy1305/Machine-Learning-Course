{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"colab":{"name":"lab08-CNN.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"TI3DgmKu4d7n"},"source":["# Laboratory 08 -  Convolutional Neural Networks\n","\n","Thus far you have seen much of pytorch functionality for building, training, validation, testing and inference with deep models in the context of fully connected networks. While these networks provide us with the means to obtain actual classification/regression output predictions, they also pose several difficulties in extracting patterns from our data. As such, fully connected layers are mainly used as final layers that summarize extracted patterns into the actual outputs, whereas hidden layers are constructed using other types of layers that are able to extract meaningful patterns. One such type of layers are convolutional layers and networks that use them are called convolutional  neural networks.  \n","\n","So in this laboratory you are going to build your own covolutional neural network. You are going to walk through all the steps necessary to build, train, test and make inferences using the CFAR10 dataset. The data set details can be found [here](https://www.cs.toronto.edu/~kriz/cifar.html) and [here](https://en.wikipedia.org/wiki/CIFAR-10).\n","\n","Without further ado, here are the imports you'll need in this lab."]},{"cell_type":"code","metadata":{"id":"v6HwdXrE4d7x","executionInfo":{"status":"ok","timestamp":1609186478003,"user_tz":-120,"elapsed":752,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"}}},"source":["import torch\n","import torch.nn.functional as F\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","\n","from torch.utils.data.sampler import SubsetRandomSampler\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from collections import OrderedDict, namedtuple\n","from itertools import product\n","torch.manual_seed(42)\n","torch.cuda.manual_seed_all(42)\n","np.random.seed(42)"],"execution_count":31,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6rtOs--x4d7y"},"source":["You'll want to train your network using the GPU's if available. In order to do this in a independent way from your machine capabilities you can use the `device` object instantiated below:"]},{"cell_type":"code","metadata":{"id":"E7U2NdxR4d7y","executionInfo":{"status":"ok","timestamp":1609186478461,"user_tz":-120,"elapsed":1197,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"}}},"source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"],"execution_count":32,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZrvwhA_N4d7z"},"source":["### Exercise 1\n","\n","Given the network architecture below, using the class notebooks implement the network and appropriate saving/loading model parameters functions."]},{"cell_type":"markdown","metadata":{"id":"NSqsj1RF4d7z"},"source":["```Python\n","Net(\n","    (features): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): BatchNorm2d(64, eps=1e-05, momentum=0.05, affine=True, track_running_stats=True)\n","    (2): ReLU(inplace=True)\n","    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (4): BatchNorm2d(128, eps=1e-05, momentum=0.05, affine=True, track_running_stats=True)\n","    (5): ReLU(inplace=True)\n","    (6): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (8): BatchNorm2d(128, eps=1e-05, momentum=0.05, affine=True, track_running_stats=True)\n","    (9): ReLU(inplace=True)\n","    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (11): BatchNorm2d(256, eps=1e-05, momentum=0.05, affine=True, track_running_stats=True)\n","    (12): ReLU(inplace=True)\n","    (13): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False))\n","(classifier): Sequential(\n","    (0): Dropout(p=0.2, inplace=True)\n","    (2): Linear(in_features=16384, out_features=1024, bias=True)\n","    (3): ReLU(inplace=True))\n",")\n","```"]},{"cell_type":"code","metadata":{"id":"ET0eDset4d70","executionInfo":{"status":"ok","timestamp":1609186478463,"user_tz":-120,"elapsed":1189,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"}}},"source":["# TODO 1.1. Implement the network architecture\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","\n","        # TODO: implement feature extraction and classifier\n","        #  - use logsoftmax on the output\n","        self.features = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n","            nn.BatchNorm2d(64, eps=1e-05, momentum=0.05, affine=True, track_running_stats=True),\n","            nn.ReLU(inplace=True),\n","\n","            nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n","            nn.BatchNorm2d(128, eps=1e-05, momentum=0.05, affine=True, track_running_stats=True),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False),\n","    \n","            nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n","            nn.BatchNorm2d(128, eps=1e-05, momentum=0.05, affine=True, track_running_stats=True),\n","            nn.ReLU(inplace=True),\n","            \n","            nn.Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n","            nn.BatchNorm2d(256, eps=1e-05, momentum=0.05, affine=True, track_running_stats=True),\n","            nn.ReLU(inplace=True),\n","            \n","            nn.MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n","        )\n","\n","        self.classifier = nn.Sequential(\n","            nn.Dropout(p=0.2, inplace=True),\n","            nn.Linear(in_features=16384, out_features=1024, bias=True),\n","            nn.ReLU(inplace=True),\n","            nn.LogSoftmax(dim = 1)\n","        )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = x.view(-1, 16384)\n","        x = self.classifier(x)\n","        return x\n","    \n","# TODO 1.2. Implement Network saving/loading functions\n","def save_model(model, checkpoint='checkpoint.pth'):\n","    # TODO: implementation goes here\n","    checkpoint_model = OrderedDict(model.named_children())\n","    checkpoint_dict = {\n","        'model': checkpoint_model,\n","        'state_dict': model.state_dict()\n","    }\n","    torch.save(checkpoint_dict, checkpoint)\n","\n","\n","\n","def load_model(model = None, checkpoint='./checkpoints/checkpoint.pth'):\n","    checkpoint_dict = torch.load(checkpoint)\n","    \n","    if model is None:\n","        model = nn.Sequential(checkpoint_dict['model'])\n","    model.load_state_dict(checkpoint_dict['state_dict'])\n","    \n","    return model\n"],"execution_count":33,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zcB76nu34d71"},"source":["### Exercise 2\n","\n","As usual you have to load in your data while applying the apropriate transformations to create the train, validate and test loaders. For the CFAR10 dataset you can use `datasets.CIFAR10()` from `torchvision` in a similar manner as you did for FashionMNIST. However, this dataset contains RGB images with 3 channales. Hence, you'll need to provide a transformation that normalizes the data across all 3 channales. So, for each of the channels use a $\\mu=0.5$ and $\\sigma=0.5$.\n","\n","Complete the TODO's below.\n","\n","**Note:** At a minimum you'll have to define the batch size hyper-parameter, so this is a good place to define all other training parameters you'll need (e.g. the number of epochs to train).  "]},{"cell_type":"code","metadata":{"id":"cRqjhzuG4d71","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609186480162,"user_tz":-120,"elapsed":2878,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"}},"outputId":"6d329742-f0f0-4263-84fd-e6369069ccf0"},"source":["# TODO 2.1. Implement train_valid_split function\n","def train_valid_split(dataset, valid_percent=0.2, batch_size=32):\n","    train_length = len(dataset)\n","    indices = list(range(train_length))\n","    # np.random.shuffle(indices)\n","    split = int(np.floor(valid_percent * train_length))\n","    train_idx, valid_idx = indices[split:], indices[:split]\n","    train_sampler = SubsetRandomSampler(train_idx)\n","    valid_sampler = SubsetRandomSampler(valid_idx)\n","    \n","    train_loader = torch.utils.data.DataLoader(\n","                      dataset,\n","                      batch_size = batch_size,\n","                      sampler = train_sampler,\n","                      num_workers = 0\n","    )\n","\n","    valid_loader = torch.utils.data.DataLoader(\n","                      dataset,\n","                      batch_size = batch_size,\n","                      sampler = valid_sampler,\n","                      num_workers = 0\n","    )\n","\n","    return train_loader, valid_loader\n","\n","# TODO 2.2. Define the batch_size (and other hyper-parameters)\n","batch_size = 64\n","epochs = 15\n","valid_percent = 0.25\n","learning_rate = 0.003\n","patience_thresold = 5\n","\n","\n","# TODO 2.3. Define the transformation used in loading the dataset\n","transforms = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])\n","\n","train_set = datasets.CIFAR10(root = 'data', train = True, download = True, transform = transforms)\n","test_set = datasets.CIFAR10(root = 'data', train = False, download = True, transform = transforms)\n","\n","# TODO 2.4. Create the train, valid and test loaders\n","train_loader, valid_loader = train_valid_split(train_set, valid_percent, batch_size)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size = batch_size, num_workers = 0)"],"execution_count":34,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rOXENkq44d72"},"source":["### Exercise 3\n","\n","You now have to implement the actual traning of the network. While this is no diffrent from training you've done in the context of fully connected networks, here you'll have to agument the training process with early stopping.\n","\n","\n","So, implement the training function such that it supports early stopping with model saving and also prints the training and valdation losses in each epoch.\n","\n","**Note:** To run the actual traning you'll also need to instantiate the appropiate loss and optimizer. Since, the network achitecture applies `LogSoftmax()` to get the final output, the loss needs to be the negative log loss, wheareas for the optimizer you can safely use `Adam()`."]},{"cell_type":"code","metadata":{"id":"5P5oRmah4d73","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609186885718,"user_tz":-120,"elapsed":408423,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"}},"outputId":"8459d29b-5e69-4e2d-a18f-6278b654ac6d"},"source":["# TODO 3.1. Implement the training function\n","def train(model, train_loader, valid_loader, criterion, optimizer, epochs=15, run = None):\n","    # TODO: implement the training loop with model saving and early stopping\n","    #   - use train_valid_split to get training and validation loaders\n","    #   - print the epoch, training loss and validation loss\n","    valid_loss_min = np.Inf\n","    patience = patience_thresold\n","    best_model_checkpoint = 'checkpoint.pth'\n","    model.to(device)\n","    best_model = None\n","    if run is not None:\n","        optimizer = optim.Adam(model.parameters(), lr = run.lr)\n","        epochs = run.epochs\n","        patience = run.patience_threshold\n","\n","    \n","    for epoch in range(epochs):\n","        train_loss = 0\n","        model.train()\n","        for data, target in train_loader:\n","            data, target = data.to(device), target.to(device)\n","            optimizer.zero_grad()\n","            output = model(data)\n","            loss = criterion(output, target)\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item() * train_loader.batch_size\n","        \n","        valid_loss = 0\n","        model.eval()\n","        with torch.no_grad():\n","            for data, target in valid_loader:\n","                data, target = data.to(device), target.to(device)\n","                output = model(data)\n","                loss = criterion(output, target)\n","                valid_loss += loss.item() * batch_size\n","\n","        train_loss = train_loss / len(train_loader.sampler)\n","        valid_loss = valid_loss / len(valid_loader.sampler)\n","\n","        print(\"Epoch: {}/{}.. \".format(epoch + 1, epochs),\n","          \"Training Loss: {:.3f}.. \".format(train_loss),\n","          \"Validation Loss: {:.3f}.. \".format(valid_loss),\n","          \"Early Stopping Patience: {}.. \".format(patience + 1))\n","        \n","        patience -= 1\n","        if valid_loss <= valid_loss_min:\n","            print('Validation loss decreased ({:.4f} --> {:.4f})..'.format(valid_loss_min, valid_loss))\n","            print('Saving to ', best_model_checkpoint,  '\\n')\n","            save_model(model, best_model_checkpoint)\n","            best_model = model\n","            valid_loss_min = valid_loss\n","            if run is not None:\n","                patience = run.patience_threshold\n","            else:\n","                patience = patience_thresold\n","        \n","        if patience == 0:\n","            break\n","           \n","    print('Done!\\n')\n","    return best_model_checkpoint, valid_loss_min, best_model\n","\n","# TODO 3.2. Instantiate the model\n","model = Net()\n","\n","# TODO 3.3. Instantiate the loss function\n","criterion = nn.NLLLoss()\n","\n","# TODO 3.4. Instantiate the optimizer\n","optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n","\n","# TODO 3.5. Train the model\n","best_model_checkpoint, _, _ = train(model, train_loader, valid_loader, criterion, optimizer, epochs=epochs)"],"execution_count":35,"outputs":[{"output_type":"stream","text":["Epoch: 1/15..  Training Loss: 3.699..  Validation Loss: 2.873..  Early Stopping Patience: 6.. \n","Validation loss decreased (inf --> 2.8735)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 2/15..  Training Loss: 2.553..  Validation Loss: 2.343..  Early Stopping Patience: 6.. \n","Validation loss decreased (2.8735 --> 2.3427)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 3/15..  Training Loss: 2.303..  Validation Loss: 2.268..  Early Stopping Patience: 6.. \n","Validation loss decreased (2.3427 --> 2.2684)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 4/15..  Training Loss: 2.153..  Validation Loss: 2.247..  Early Stopping Patience: 6.. \n","Validation loss decreased (2.2684 --> 2.2468)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 5/15..  Training Loss: 2.042..  Validation Loss: 2.206..  Early Stopping Patience: 6.. \n","Validation loss decreased (2.2468 --> 2.2056)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 6/15..  Training Loss: 1.954..  Validation Loss: 2.088..  Early Stopping Patience: 6.. \n","Validation loss decreased (2.2056 --> 2.0879)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 7/15..  Training Loss: 1.870..  Validation Loss: 2.133..  Early Stopping Patience: 6.. \n","Epoch: 8/15..  Training Loss: 1.791..  Validation Loss: 2.082..  Early Stopping Patience: 5.. \n","Validation loss decreased (2.0879 --> 2.0817)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 9/15..  Training Loss: 1.717..  Validation Loss: 2.050..  Early Stopping Patience: 6.. \n","Validation loss decreased (2.0817 --> 2.0502)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 10/15..  Training Loss: 1.665..  Validation Loss: 2.200..  Early Stopping Patience: 6.. \n","Epoch: 11/15..  Training Loss: 1.612..  Validation Loss: 2.140..  Early Stopping Patience: 5.. \n","Epoch: 12/15..  Training Loss: 1.577..  Validation Loss: 2.159..  Early Stopping Patience: 4.. \n","Epoch: 13/15..  Training Loss: 1.549..  Validation Loss: 2.225..  Early Stopping Patience: 3.. \n","Epoch: 14/15..  Training Loss: 1.533..  Validation Loss: 2.358..  Early Stopping Patience: 2.. \n","Done!\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aWRllrSc4d74"},"source":["### Exercise 4\n","\n","Up to now we generally computed (and printed) the overall loss and accuracy for the test set. However, a more detailed summary such as the prediction accuracy for each class is often more useful in evaluating the behavior of the a network architecture. You can do this by first computing the correct predictions in each mini-batch and then use this to compute the total number correct predictions for each class:\n","\n","```Python\n","\n","# This gives you a tensor of 0/1, where 1 corresponds to correct predictions\n","correct_outputs = pred.eq(target.data.view_as(pred))\n","\n","...\n","\n","# Computing the number of correct predictions in each class and\n","# the number of examples in each class (class_correct / class_total) \n","for i in range(len(images))\n","    class_correct[label] += correct_outputs[i].item()\n","    class_total[label] += 1\n","\n","...\n","\n","# Computing class i accuracy \n","class_accuracy = class_correct[i] / class_total[i]\n","\n","```\n","\n","**Note:** To actually compare output probability distribution with the one-hot encoded target you can transform each output by using the `torch.max()` method on the appropriate dimension, i.e. supplying the correct`dim=` argument. \n","\n","So, in the exercise below implement the test function and print the overall loss and accuracy, plus the accuracy for each class.\n"]},{"cell_type":"code","metadata":{"id":"bItlpdRO4d75","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609186889079,"user_tz":-120,"elapsed":411772,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"}},"outputId":"65f827cc-fe0d-4cff-e0a3-4336ed889cbf"},"source":["# TODO 4.1. Implement the test loop and print relavant information: \n","# test loss, class accuracy, overall accuracy\n","\n","def test(model, test_loader, criterion):\n","\n","    class_correct = list(0. for i in range(len(test_set.classes)))\n","    class_total = list(0. for i in range(len(test_set.classes)))\n","\n","    # TODO: implement the test loop\n","    #   - print relavant information: test loss, class accuracy, overall accuracy\n","\n","    test_loss = 0\n","    model.eval()\n","    for data, target in test_loader:\n","        data, target = data.to(device), target.to(device)\n","        output = model(data)\n","        loss = criterion(output, target)\n","        test_loss += loss.item() * batch_size\n","\n","        ps = torch.exp(output)\n","        _, pred = torch.max(ps, dim=1)\n","        correct_tensor = pred.eq(target.data.view_as(pred))\n","\n","        if torch.cuda.is_available():\n","            correct = np.squeeze(correct_tensor.cpu().numpy())\n","        else:\n","            correct = np.squeeze(correct_tensor.numpy())\n","\n","        for i in range(len(data)):\n","            label = target.data[i].item()\n","            class_correct[label] += correct[i].item()\n","            class_total[label] += 1\n","    \n","    # average test loss\n","    test_loss = test_loss/len(test_loader.dataset)\n","    print('Test Loss: {:.4f}'.format(test_loss))\n","\n","    for i in range(len(test_set.classes)):\n","        if class_total[i] > 0:\n","            print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n","                test_set.classes[i], 100 * class_correct[i] / class_total[i],\n","                np.sum(class_correct[i]), np.sum(class_total[i])))\n","        else:\n","            print('Test Accuracy of %5s: N/A (no training examples)' % (test.classes[i]))\n","\n","    print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n","        100. * np.sum(class_correct) / np.sum(class_total),\n","        np.sum(class_correct), np.sum(class_total)))\n","    \n","model = Net()\n","model = load_model(model, best_model_checkpoint).to(device)\n","test(model, test_loader, criterion)"],"execution_count":36,"outputs":[{"output_type":"stream","text":["Test Loss: 2.0700\n","Test Accuracy of airplane: 83% (832/1000)\n","Test Accuracy of automobile:  0% ( 0/1000)\n","Test Accuracy of  bird: 70% (705/1000)\n","Test Accuracy of   cat:  0% ( 0/1000)\n","Test Accuracy of  deer: 75% (755/1000)\n","Test Accuracy of   dog: 85% (856/1000)\n","Test Accuracy of  frog: 87% (874/1000)\n","Test Accuracy of horse: 86% (866/1000)\n","Test Accuracy of  ship: 88% (882/1000)\n","Test Accuracy of truck: 87% (875/1000)\n","\n","Test Accuracy (Overall): 66% (6645/10000)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"w8_JwzVf4d75"},"source":["### Exercise 5\n","\n","As you have noticed, our network performs poorly. To determine whether this is a hyper-parameter issue or we actually need to make changes in the network architecture, you can do a hyper-parameter search and view how the network preforms for each combinations of parameters in each run. So, in this exercise you will have to:\n","- Implement a RunBuilder class that outputs a list of parameters describing each run.\n","- Refactor the training function such that it accepts a dictionary of parameters and use this dictionary to do the actual training.\n","- The training function must return the absolute best model checkpoint among all the training runs. While during training we selected the best model based on the validation error, here you'll want to also keep track of the best model in each run durring testing."]},{"cell_type":"code","metadata":{"id":"kQalOZ_54d76","executionInfo":{"status":"ok","timestamp":1609186889082,"user_tz":-120,"elapsed":411767,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"}}},"source":["# TODO 5.1. Implement traning and test with hyper-parameter search\n","class RunBuilder():\n","    @staticmethod\n","    def get_runs(params):\n","        Run = namedtuple('Run', params.keys())\n","\n","        runs = []\n","        for v in product(*params.values()):\n","            runs.append(Run(*v))\n","        return runs"],"execution_count":37,"outputs":[]},{"cell_type":"code","metadata":{"id":"u9x4ldIq4d76","executionInfo":{"status":"ok","timestamp":1609186889085,"user_tz":-120,"elapsed":411763,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"}}},"source":["params = OrderedDict(\n","    epochs = [30],\n","    lr = [0.0003, 0.003, 0.03],\n","    batch_size = [32, 64],\n","    patience_threshold = [6, 10]\n",")\n","runs = RunBuilder.get_runs(params)"],"execution_count":38,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZkKcC6BnG9Xl","executionInfo":{"status":"ok","timestamp":1609194371289,"user_tz":-120,"elapsed":7893960,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"}},"outputId":"a6f8b337-f33a-495f-989b-1c16d0bee426"},"source":["min_valid_loss = np.Inf\n","best_run = None\n","best_model = None\n","for run in runs:\n","    model = Net()\n","    criterion = nn.NLLLoss()\n","    optimizer = optim.Adam(model.parameters(), lr = run.lr)\n","    batch_size = run.batch_size\n","    train_loader, valid_loader = train_valid_split(train_set, valid_percent, batch_size)\n","    best_model_checkpoint, valid_loss, crt_model = train(model, train_loader, valid_loader, criterion, optimizer, epochs = epochs, run = run)\n","    if valid_loss < min_valid_loss:\n","        min_valid_loss = valid_loss\n","        best_model = crt_model\n","        best_run = run\n","    print(str(runs.index(run)) + \" out of \" + str(len(runs)) + \": \" + str(run) + \" \" + str(min_valid_loss))"],"execution_count":39,"outputs":[{"output_type":"stream","text":["Epoch: 1/30..  Training Loss: 3.330..  Validation Loss: 3.062..  Early Stopping Patience: 7.. \n","Validation loss decreased (inf --> 3.0615)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 2/30..  Training Loss: 2.960..  Validation Loss: 2.961..  Early Stopping Patience: 7.. \n","Validation loss decreased (3.0615 --> 2.9609)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 3/30..  Training Loss: 2.826..  Validation Loss: 2.836..  Early Stopping Patience: 7.. \n","Validation loss decreased (2.9609 --> 2.8362)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 4/30..  Training Loss: 2.729..  Validation Loss: 2.857..  Early Stopping Patience: 7.. \n","Epoch: 5/30..  Training Loss: 2.647..  Validation Loss: 2.778..  Early Stopping Patience: 6.. \n","Validation loss decreased (2.8362 --> 2.7776)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 6/30..  Training Loss: 2.580..  Validation Loss: 2.690..  Early Stopping Patience: 7.. \n","Validation loss decreased (2.7776 --> 2.6905)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 7/30..  Training Loss: 2.523..  Validation Loss: 2.710..  Early Stopping Patience: 7.. \n","Epoch: 8/30..  Training Loss: 2.464..  Validation Loss: 2.676..  Early Stopping Patience: 6.. \n","Validation loss decreased (2.6905 --> 2.6756)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 9/30..  Training Loss: 2.418..  Validation Loss: 2.693..  Early Stopping Patience: 7.. \n","Epoch: 10/30..  Training Loss: 2.364..  Validation Loss: 2.708..  Early Stopping Patience: 6.. \n","Epoch: 11/30..  Training Loss: 2.328..  Validation Loss: 2.710..  Early Stopping Patience: 5.. \n","Epoch: 12/30..  Training Loss: 2.294..  Validation Loss: 2.793..  Early Stopping Patience: 4.. \n","Epoch: 13/30..  Training Loss: 2.259..  Validation Loss: 2.763..  Early Stopping Patience: 3.. \n","Epoch: 14/30..  Training Loss: 2.239..  Validation Loss: 2.742..  Early Stopping Patience: 2.. \n","Done!\n","\n","0 out of 12: Run(epochs=30, lr=0.0003, batch_size=32, patience_threshold=6) 2.675620657043457\n","Epoch: 1/30..  Training Loss: 2.077..  Validation Loss: 1.744..  Early Stopping Patience: 11.. \n","Validation loss decreased (inf --> 1.7439)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 2/30..  Training Loss: 1.636..  Validation Loss: 1.638..  Early Stopping Patience: 11.. \n","Validation loss decreased (1.7439 --> 1.6383)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 3/30..  Training Loss: 1.474..  Validation Loss: 1.540..  Early Stopping Patience: 11.. \n","Validation loss decreased (1.6383 --> 1.5400)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 4/30..  Training Loss: 1.374..  Validation Loss: 1.467..  Early Stopping Patience: 11.. \n","Validation loss decreased (1.5400 --> 1.4671)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 5/30..  Training Loss: 1.293..  Validation Loss: 1.386..  Early Stopping Patience: 11.. \n","Validation loss decreased (1.4671 --> 1.3859)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 6/30..  Training Loss: 1.207..  Validation Loss: 1.448..  Early Stopping Patience: 11.. \n","Epoch: 7/30..  Training Loss: 1.137..  Validation Loss: 1.439..  Early Stopping Patience: 10.. \n","Epoch: 8/30..  Training Loss: 1.087..  Validation Loss: 1.333..  Early Stopping Patience: 9.. \n","Validation loss decreased (1.3859 --> 1.3333)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 9/30..  Training Loss: 1.019..  Validation Loss: 1.441..  Early Stopping Patience: 11.. \n","Epoch: 10/30..  Training Loss: 0.973..  Validation Loss: 1.399..  Early Stopping Patience: 10.. \n","Epoch: 11/30..  Training Loss: 0.932..  Validation Loss: 1.417..  Early Stopping Patience: 9.. \n","Epoch: 12/30..  Training Loss: 0.904..  Validation Loss: 1.407..  Early Stopping Patience: 8.. \n","Epoch: 13/30..  Training Loss: 0.874..  Validation Loss: 1.519..  Early Stopping Patience: 7.. \n","Epoch: 14/30..  Training Loss: 0.845..  Validation Loss: 1.429..  Early Stopping Patience: 6.. \n","Epoch: 15/30..  Training Loss: 0.830..  Validation Loss: 1.439..  Early Stopping Patience: 5.. \n","Epoch: 16/30..  Training Loss: 0.816..  Validation Loss: 1.461..  Early Stopping Patience: 4.. \n","Epoch: 17/30..  Training Loss: 0.798..  Validation Loss: 1.469..  Early Stopping Patience: 3.. \n","Epoch: 18/30..  Training Loss: 0.797..  Validation Loss: 1.602..  Early Stopping Patience: 2.. \n","Done!\n","\n","1 out of 12: Run(epochs=30, lr=0.0003, batch_size=32, patience_threshold=10) 1.3333240259933472\n","Epoch: 1/30..  Training Loss: 1.302..  Validation Loss: 1.014..  Early Stopping Patience: 7.. \n","Validation loss decreased (inf --> 1.0142)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 2/30..  Training Loss: 0.893..  Validation Loss: 0.932..  Early Stopping Patience: 7.. \n","Validation loss decreased (1.0142 --> 0.9324)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 3/30..  Training Loss: 0.744..  Validation Loss: 1.074..  Early Stopping Patience: 7.. \n","Epoch: 4/30..  Training Loss: 0.644..  Validation Loss: 0.838..  Early Stopping Patience: 6.. \n","Validation loss decreased (0.9324 --> 0.8384)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 5/30..  Training Loss: 0.568..  Validation Loss: 0.785..  Early Stopping Patience: 7.. \n","Validation loss decreased (0.8384 --> 0.7849)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 6/30..  Training Loss: 0.479..  Validation Loss: 0.705..  Early Stopping Patience: 7.. \n","Validation loss decreased (0.7849 --> 0.7048)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 7/30..  Training Loss: 0.420..  Validation Loss: 1.172..  Early Stopping Patience: 7.. \n","Epoch: 8/30..  Training Loss: 0.365..  Validation Loss: 0.711..  Early Stopping Patience: 6.. \n","Epoch: 9/30..  Training Loss: 0.310..  Validation Loss: 0.732..  Early Stopping Patience: 5.. \n","Epoch: 10/30..  Training Loss: 0.268..  Validation Loss: 0.787..  Early Stopping Patience: 4.. \n","Epoch: 11/30..  Training Loss: 0.223..  Validation Loss: 0.919..  Early Stopping Patience: 3.. \n","Epoch: 12/30..  Training Loss: 0.199..  Validation Loss: 0.914..  Early Stopping Patience: 2.. \n","Done!\n","\n","2 out of 12: Run(epochs=30, lr=0.0003, batch_size=64, patience_threshold=6) 0.7048410765075683\n","Epoch: 1/30..  Training Loss: 1.309..  Validation Loss: 1.164..  Early Stopping Patience: 11.. \n","Validation loss decreased (inf --> 1.1640)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 2/30..  Training Loss: 0.887..  Validation Loss: 0.902..  Early Stopping Patience: 11.. \n","Validation loss decreased (1.1640 --> 0.9023)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 3/30..  Training Loss: 0.750..  Validation Loss: 1.018..  Early Stopping Patience: 11.. \n","Epoch: 4/30..  Training Loss: 0.652..  Validation Loss: 0.760..  Early Stopping Patience: 10.. \n","Validation loss decreased (0.9023 --> 0.7600)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 5/30..  Training Loss: 0.569..  Validation Loss: 0.881..  Early Stopping Patience: 11.. \n","Epoch: 6/30..  Training Loss: 0.473..  Validation Loss: 0.707..  Early Stopping Patience: 10.. \n","Validation loss decreased (0.7600 --> 0.7068)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 7/30..  Training Loss: 0.435..  Validation Loss: 0.687..  Early Stopping Patience: 11.. \n","Validation loss decreased (0.7068 --> 0.6869)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 8/30..  Training Loss: 0.376..  Validation Loss: 0.749..  Early Stopping Patience: 11.. \n","Epoch: 9/30..  Training Loss: 0.317..  Validation Loss: 0.806..  Early Stopping Patience: 10.. \n","Epoch: 10/30..  Training Loss: 0.275..  Validation Loss: 0.848..  Early Stopping Patience: 9.. \n","Epoch: 11/30..  Training Loss: 0.227..  Validation Loss: 0.699..  Early Stopping Patience: 8.. \n","Epoch: 12/30..  Training Loss: 0.210..  Validation Loss: 0.724..  Early Stopping Patience: 7.. \n","Epoch: 13/30..  Training Loss: 0.173..  Validation Loss: 0.739..  Early Stopping Patience: 6.. \n","Epoch: 14/30..  Training Loss: 0.157..  Validation Loss: 0.843..  Early Stopping Patience: 5.. \n","Epoch: 15/30..  Training Loss: 0.135..  Validation Loss: 0.693..  Early Stopping Patience: 4.. \n","Epoch: 16/30..  Training Loss: 0.129..  Validation Loss: 1.015..  Early Stopping Patience: 3.. \n","Epoch: 17/30..  Training Loss: 0.104..  Validation Loss: 0.741..  Early Stopping Patience: 2.. \n","Done!\n","\n","3 out of 12: Run(epochs=30, lr=0.0003, batch_size=64, patience_threshold=10) 0.6869018225097656\n","Epoch: 1/30..  Training Loss: 5.373..  Validation Loss: 4.865..  Early Stopping Patience: 7.. \n","Validation loss decreased (inf --> 4.8646)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 2/30..  Training Loss: 4.775..  Validation Loss: 4.666..  Early Stopping Patience: 7.. \n","Validation loss decreased (4.8646 --> 4.6663)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 3/30..  Training Loss: 4.642..  Validation Loss: 4.598..  Early Stopping Patience: 7.. \n","Validation loss decreased (4.6663 --> 4.5983)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 4/30..  Training Loss: 4.564..  Validation Loss: 4.552..  Early Stopping Patience: 7.. \n","Validation loss decreased (4.5983 --> 4.5524)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 5/30..  Training Loss: 4.506..  Validation Loss: 4.611..  Early Stopping Patience: 7.. \n","Epoch: 6/30..  Training Loss: 4.455..  Validation Loss: 4.530..  Early Stopping Patience: 6.. \n","Validation loss decreased (4.5524 --> 4.5302)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 7/30..  Training Loss: 4.415..  Validation Loss: 4.552..  Early Stopping Patience: 7.. \n","Epoch: 8/30..  Training Loss: 4.377..  Validation Loss: 4.497..  Early Stopping Patience: 6.. \n","Validation loss decreased (4.5302 --> 4.4970)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 9/30..  Training Loss: 4.348..  Validation Loss: 4.524..  Early Stopping Patience: 7.. \n","Epoch: 10/30..  Training Loss: 4.313..  Validation Loss: 4.526..  Early Stopping Patience: 6.. \n","Epoch: 11/30..  Training Loss: 4.296..  Validation Loss: 4.558..  Early Stopping Patience: 5.. \n","Epoch: 12/30..  Training Loss: 4.277..  Validation Loss: 4.553..  Early Stopping Patience: 4.. \n","Epoch: 13/30..  Training Loss: 4.264..  Validation Loss: 4.545..  Early Stopping Patience: 3.. \n","Epoch: 14/30..  Training Loss: 4.256..  Validation Loss: 4.578..  Early Stopping Patience: 2.. \n","Done!\n","\n","4 out of 12: Run(epochs=30, lr=0.003, batch_size=32, patience_threshold=6) 0.6869018225097656\n","Epoch: 1/30..  Training Loss: 6.999..  Validation Loss: 6.938..  Early Stopping Patience: 11.. \n","Validation loss decreased (inf --> 6.9381)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 2/30..  Training Loss: 6.932..  Validation Loss: 6.938..  Early Stopping Patience: 11.. \n","Validation loss decreased (6.9381 --> 6.9381)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 3/30..  Training Loss: 6.931..  Validation Loss: 6.938..  Early Stopping Patience: 11.. \n","Validation loss decreased (6.9381 --> 6.9381)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 4/30..  Training Loss: 6.932..  Validation Loss: 6.938..  Early Stopping Patience: 11.. \n","Validation loss decreased (6.9381 --> 6.9381)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 5/30..  Training Loss: 6.932..  Validation Loss: 6.938..  Early Stopping Patience: 11.. \n","Validation loss decreased (6.9381 --> 6.9381)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 6/30..  Training Loss: 6.932..  Validation Loss: 6.938..  Early Stopping Patience: 11.. \n","Validation loss decreased (6.9381 --> 6.9381)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 7/30..  Training Loss: 6.932..  Validation Loss: 6.938..  Early Stopping Patience: 11.. \n","Validation loss decreased (6.9381 --> 6.9381)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 8/30..  Training Loss: 6.932..  Validation Loss: 6.938..  Early Stopping Patience: 11.. \n","Validation loss decreased (6.9381 --> 6.9381)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 9/30..  Training Loss: 6.932..  Validation Loss: 6.938..  Early Stopping Patience: 11.. \n","Validation loss decreased (6.9381 --> 6.9381)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 10/30..  Training Loss: 6.932..  Validation Loss: 6.938..  Early Stopping Patience: 11.. \n","Validation loss decreased (6.9381 --> 6.9381)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 11/30..  Training Loss: 6.932..  Validation Loss: 6.938..  Early Stopping Patience: 11.. \n","Validation loss decreased (6.9381 --> 6.9381)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 12/30..  Training Loss: 6.932..  Validation Loss: 6.938..  Early Stopping Patience: 11.. \n","Validation loss decreased (6.9381 --> 6.9381)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 13/30..  Training Loss: 6.932..  Validation Loss: 6.938..  Early Stopping Patience: 11.. \n","Validation loss decreased (6.9381 --> 6.9381)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 14/30..  Training Loss: 6.932..  Validation Loss: 6.938..  Early Stopping Patience: 11.. \n","Validation loss decreased (6.9381 --> 6.9381)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 15/30..  Training Loss: 6.932..  Validation Loss: 6.938..  Early Stopping Patience: 11.. \n","Validation loss decreased (6.9381 --> 6.9381)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 16/30..  Training Loss: 6.932..  Validation Loss: 6.938..  Early Stopping Patience: 11.. \n","Validation loss decreased (6.9381 --> 6.9381)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 17/30..  Training Loss: 6.932..  Validation Loss: 6.938..  Early Stopping Patience: 11.. \n","Validation loss decreased (6.9381 --> 6.9381)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 18/30..  Training Loss: 6.932..  Validation Loss: 6.938..  Early Stopping Patience: 11.. \n","Validation loss decreased (6.9381 --> 6.9381)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 19/30..  Training Loss: 6.932..  Validation Loss: 6.938..  Early Stopping Patience: 11.. \n","Validation loss decreased (6.9381 --> 6.9381)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 20/30..  Training Loss: 6.932..  Validation Loss: 6.938..  Early Stopping Patience: 11.. \n","Validation loss decreased (6.9381 --> 6.9381)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 21/30..  Training Loss: 6.932..  Validation Loss: 6.938..  Early Stopping Patience: 11.. \n","Validation loss decreased (6.9381 --> 6.9381)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 22/30..  Training Loss: 6.932..  Validation Loss: 6.938..  Early Stopping Patience: 11.. \n","Validation loss decreased (6.9381 --> 6.9381)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 23/30..  Training Loss: 6.932..  Validation Loss: 6.938..  Early Stopping Patience: 11.. \n","Validation loss decreased (6.9381 --> 6.9381)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 24/30..  Training Loss: 6.932..  Validation Loss: 6.938..  Early Stopping Patience: 11.. \n","Validation loss decreased (6.9381 --> 6.9381)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 25/30..  Training Loss: 6.932..  Validation Loss: 6.938..  Early Stopping Patience: 11.. \n","Validation loss decreased (6.9381 --> 6.9381)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 26/30..  Training Loss: 6.932..  Validation Loss: 6.938..  Early Stopping Patience: 11.. \n","Validation loss decreased (6.9381 --> 6.9381)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 27/30..  Training Loss: 6.932..  Validation Loss: 6.938..  Early Stopping Patience: 11.. \n","Validation loss decreased (6.9381 --> 6.9381)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 28/30..  Training Loss: 6.932..  Validation Loss: 6.938..  Early Stopping Patience: 11.. \n","Validation loss decreased (6.9381 --> 6.9381)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 29/30..  Training Loss: 6.932..  Validation Loss: 6.938..  Early Stopping Patience: 11.. \n","Validation loss decreased (6.9381 --> 6.9381)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 30/30..  Training Loss: 6.932..  Validation Loss: 6.938..  Early Stopping Patience: 11.. \n","Validation loss decreased (6.9381 --> 6.9381)..\n","Saving to  checkpoint.pth \n","\n","Done!\n","\n","5 out of 12: Run(epochs=30, lr=0.003, batch_size=32, patience_threshold=10) 0.6869018225097656\n","Epoch: 1/30..  Training Loss: 3.473..  Validation Loss: 2.832..  Early Stopping Patience: 7.. \n","Validation loss decreased (inf --> 2.8321)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 2/30..  Training Loss: 2.526..  Validation Loss: 2.398..  Early Stopping Patience: 7.. \n","Validation loss decreased (2.8321 --> 2.3979)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 3/30..  Training Loss: 2.289..  Validation Loss: 2.281..  Early Stopping Patience: 7.. \n","Validation loss decreased (2.3979 --> 2.2806)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 4/30..  Training Loss: 2.150..  Validation Loss: 2.120..  Early Stopping Patience: 7.. \n","Validation loss decreased (2.2806 --> 2.1201)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 5/30..  Training Loss: 2.044..  Validation Loss: 2.124..  Early Stopping Patience: 7.. \n","Epoch: 6/30..  Training Loss: 1.942..  Validation Loss: 2.073..  Early Stopping Patience: 6.. \n","Validation loss decreased (2.1201 --> 2.0732)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 7/30..  Training Loss: 1.856..  Validation Loss: 2.138..  Early Stopping Patience: 7.. \n","Epoch: 8/30..  Training Loss: 1.778..  Validation Loss: 2.033..  Early Stopping Patience: 6.. \n","Validation loss decreased (2.0732 --> 2.0332)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 9/30..  Training Loss: 1.711..  Validation Loss: 2.041..  Early Stopping Patience: 7.. \n","Epoch: 10/30..  Training Loss: 1.648..  Validation Loss: 2.095..  Early Stopping Patience: 6.. \n","Epoch: 11/30..  Training Loss: 1.611..  Validation Loss: 2.090..  Early Stopping Patience: 5.. \n","Epoch: 12/30..  Training Loss: 1.582..  Validation Loss: 2.200..  Early Stopping Patience: 4.. \n","Epoch: 13/30..  Training Loss: 1.548..  Validation Loss: 2.220..  Early Stopping Patience: 3.. \n","Epoch: 14/30..  Training Loss: 1.528..  Validation Loss: 2.331..  Early Stopping Patience: 2.. \n","Done!\n","\n","6 out of 12: Run(epochs=30, lr=0.003, batch_size=64, patience_threshold=6) 0.6869018225097656\n","Epoch: 1/30..  Training Loss: 2.914..  Validation Loss: 1.917..  Early Stopping Patience: 11.. \n","Validation loss decreased (inf --> 1.9174)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 2/30..  Training Loss: 1.823..  Validation Loss: 1.775..  Early Stopping Patience: 11.. \n","Validation loss decreased (1.9174 --> 1.7752)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 3/30..  Training Loss: 1.599..  Validation Loss: 1.557..  Early Stopping Patience: 11.. \n","Validation loss decreased (1.7752 --> 1.5569)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 4/30..  Training Loss: 1.433..  Validation Loss: 1.429..  Early Stopping Patience: 11.. \n","Validation loss decreased (1.5569 --> 1.4291)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 5/30..  Training Loss: 1.337..  Validation Loss: 1.550..  Early Stopping Patience: 11.. \n","Epoch: 6/30..  Training Loss: 1.242..  Validation Loss: 1.383..  Early Stopping Patience: 10.. \n","Validation loss decreased (1.4291 --> 1.3835)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 7/30..  Training Loss: 1.155..  Validation Loss: 1.364..  Early Stopping Patience: 11.. \n","Validation loss decreased (1.3835 --> 1.3644)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 8/30..  Training Loss: 1.089..  Validation Loss: 1.342..  Early Stopping Patience: 11.. \n","Validation loss decreased (1.3644 --> 1.3423)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 9/30..  Training Loss: 1.022..  Validation Loss: 1.397..  Early Stopping Patience: 11.. \n","Epoch: 10/30..  Training Loss: 0.967..  Validation Loss: 1.454..  Early Stopping Patience: 10.. \n","Epoch: 11/30..  Training Loss: 0.928..  Validation Loss: 1.518..  Early Stopping Patience: 9.. \n","Epoch: 12/30..  Training Loss: 0.890..  Validation Loss: 1.508..  Early Stopping Patience: 8.. \n","Epoch: 13/30..  Training Loss: 0.862..  Validation Loss: 1.553..  Early Stopping Patience: 7.. \n","Epoch: 14/30..  Training Loss: 0.841..  Validation Loss: 1.523..  Early Stopping Patience: 6.. \n","Epoch: 15/30..  Training Loss: 0.834..  Validation Loss: 1.591..  Early Stopping Patience: 5.. \n","Epoch: 16/30..  Training Loss: 0.810..  Validation Loss: 1.623..  Early Stopping Patience: 4.. \n","Epoch: 17/30..  Training Loss: 0.799..  Validation Loss: 1.624..  Early Stopping Patience: 3.. \n","Epoch: 18/30..  Training Loss: 0.799..  Validation Loss: 1.679..  Early Stopping Patience: 2.. \n","Done!\n","\n","7 out of 12: Run(epochs=30, lr=0.003, batch_size=64, patience_threshold=10) 0.6869018225097656\n","Epoch: 1/30..  Training Loss: 5.617..  Validation Loss: 3.735..  Early Stopping Patience: 7.. \n","Validation loss decreased (inf --> 3.7351)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 2/30..  Training Loss: 3.681..  Validation Loss: 3.509..  Early Stopping Patience: 7.. \n","Validation loss decreased (3.7351 --> 3.5085)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 3/30..  Training Loss: 3.515..  Validation Loss: 3.413..  Early Stopping Patience: 7.. \n","Validation loss decreased (3.5085 --> 3.4126)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 4/30..  Training Loss: 3.450..  Validation Loss: 3.908..  Early Stopping Patience: 7.. \n","Epoch: 5/30..  Training Loss: 3.387..  Validation Loss: 3.333..  Early Stopping Patience: 6.. \n","Validation loss decreased (3.4126 --> 3.3325)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 6/30..  Training Loss: 3.336..  Validation Loss: 3.320..  Early Stopping Patience: 7.. \n","Validation loss decreased (3.3325 --> 3.3199)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 7/30..  Training Loss: 3.293..  Validation Loss: 3.603..  Early Stopping Patience: 7.. \n","Epoch: 8/30..  Training Loss: 3.259..  Validation Loss: 3.273..  Early Stopping Patience: 6.. \n","Validation loss decreased (3.3199 --> 3.2727)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 9/30..  Training Loss: 3.226..  Validation Loss: 3.171..  Early Stopping Patience: 7.. \n","Validation loss decreased (3.2727 --> 3.1708)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 10/30..  Training Loss: 3.192..  Validation Loss: 3.126..  Early Stopping Patience: 7.. \n","Validation loss decreased (3.1708 --> 3.1262)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 11/30..  Training Loss: 3.162..  Validation Loss: 3.107..  Early Stopping Patience: 7.. \n","Validation loss decreased (3.1262 --> 3.1069)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 12/30..  Training Loss: 3.133..  Validation Loss: 3.112..  Early Stopping Patience: 7.. \n","Epoch: 13/30..  Training Loss: 3.118..  Validation Loss: 3.101..  Early Stopping Patience: 6.. \n","Validation loss decreased (3.1069 --> 3.1011)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 14/30..  Training Loss: 3.089..  Validation Loss: 3.099..  Early Stopping Patience: 7.. \n","Validation loss decreased (3.1011 --> 3.0990)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 15/30..  Training Loss: 3.073..  Validation Loss: 3.193..  Early Stopping Patience: 7.. \n","Epoch: 16/30..  Training Loss: 3.050..  Validation Loss: 3.149..  Early Stopping Patience: 6.. \n","Epoch: 17/30..  Training Loss: 3.023..  Validation Loss: 3.057..  Early Stopping Patience: 5.. \n","Validation loss decreased (3.0990 --> 3.0571)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 18/30..  Training Loss: 3.013..  Validation Loss: 3.066..  Early Stopping Patience: 7.. \n","Epoch: 19/30..  Training Loss: 3.007..  Validation Loss: 3.038..  Early Stopping Patience: 6.. \n","Validation loss decreased (3.0571 --> 3.0375)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 20/30..  Training Loss: 2.978..  Validation Loss: 3.083..  Early Stopping Patience: 7.. \n","Epoch: 21/30..  Training Loss: 2.966..  Validation Loss: 3.041..  Early Stopping Patience: 6.. \n","Epoch: 22/30..  Training Loss: 2.944..  Validation Loss: 3.077..  Early Stopping Patience: 5.. \n","Epoch: 23/30..  Training Loss: 2.935..  Validation Loss: 3.079..  Early Stopping Patience: 4.. \n","Epoch: 24/30..  Training Loss: 2.912..  Validation Loss: 3.033..  Early Stopping Patience: 3.. \n","Validation loss decreased (3.0375 --> 3.0326)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 25/30..  Training Loss: 2.903..  Validation Loss: 3.031..  Early Stopping Patience: 7.. \n","Validation loss decreased (3.0326 --> 3.0315)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 26/30..  Training Loss: 2.887..  Validation Loss: 3.033..  Early Stopping Patience: 7.. \n","Epoch: 27/30..  Training Loss: 2.882..  Validation Loss: 3.036..  Early Stopping Patience: 6.. \n","Epoch: 28/30..  Training Loss: 2.863..  Validation Loss: 3.022..  Early Stopping Patience: 5.. \n","Validation loss decreased (3.0315 --> 3.0218)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 29/30..  Training Loss: 2.859..  Validation Loss: 3.099..  Early Stopping Patience: 7.. \n","Epoch: 30/30..  Training Loss: 2.839..  Validation Loss: 3.050..  Early Stopping Patience: 6.. \n","Done!\n","\n","8 out of 12: Run(epochs=30, lr=0.03, batch_size=32, patience_threshold=6) 0.6869018225097656\n","Epoch: 1/30..  Training Loss: 6.902..  Validation Loss: 4.705..  Early Stopping Patience: 11.. \n","Validation loss decreased (inf --> 4.7049)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 2/30..  Training Loss: 4.589..  Validation Loss: 4.561..  Early Stopping Patience: 11.. \n","Validation loss decreased (4.7049 --> 4.5613)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 3/30..  Training Loss: 4.535..  Validation Loss: 4.455..  Early Stopping Patience: 11.. \n","Validation loss decreased (4.5613 --> 4.4550)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 4/30..  Training Loss: 4.482..  Validation Loss: 4.431..  Early Stopping Patience: 11.. \n","Validation loss decreased (4.4550 --> 4.4311)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 5/30..  Training Loss: 4.424..  Validation Loss: 4.330..  Early Stopping Patience: 11.. \n","Validation loss decreased (4.4311 --> 4.3299)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 6/30..  Training Loss: 4.374..  Validation Loss: 4.276..  Early Stopping Patience: 11.. \n","Validation loss decreased (4.3299 --> 4.2763)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 7/30..  Training Loss: 4.317..  Validation Loss: 4.332..  Early Stopping Patience: 11.. \n","Epoch: 8/30..  Training Loss: 4.288..  Validation Loss: 4.233..  Early Stopping Patience: 10.. \n","Validation loss decreased (4.2763 --> 4.2331)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 9/30..  Training Loss: 4.241..  Validation Loss: 4.175..  Early Stopping Patience: 11.. \n","Validation loss decreased (4.2331 --> 4.1752)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 10/30..  Training Loss: 4.215..  Validation Loss: 4.187..  Early Stopping Patience: 11.. \n","Epoch: 11/30..  Training Loss: 4.193..  Validation Loss: 4.149..  Early Stopping Patience: 10.. \n","Validation loss decreased (4.1752 --> 4.1494)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 12/30..  Training Loss: 4.167..  Validation Loss: 4.142..  Early Stopping Patience: 11.. \n","Validation loss decreased (4.1494 --> 4.1417)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 13/30..  Training Loss: 4.141..  Validation Loss: 4.151..  Early Stopping Patience: 11.. \n","Epoch: 14/30..  Training Loss: 4.120..  Validation Loss: 4.192..  Early Stopping Patience: 10.. \n","Epoch: 15/30..  Training Loss: 4.103..  Validation Loss: 4.100..  Early Stopping Patience: 9.. \n","Validation loss decreased (4.1417 --> 4.1003)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 16/30..  Training Loss: 4.082..  Validation Loss: 4.129..  Early Stopping Patience: 11.. \n","Epoch: 17/30..  Training Loss: 4.071..  Validation Loss: 4.108..  Early Stopping Patience: 10.. \n","Epoch: 18/30..  Training Loss: 4.060..  Validation Loss: 4.111..  Early Stopping Patience: 9.. \n","Epoch: 19/30..  Training Loss: 4.044..  Validation Loss: 4.144..  Early Stopping Patience: 8.. \n","Epoch: 20/30..  Training Loss: 4.027..  Validation Loss: 4.105..  Early Stopping Patience: 7.. \n","Epoch: 21/30..  Training Loss: 4.011..  Validation Loss: 4.107..  Early Stopping Patience: 6.. \n","Epoch: 22/30..  Training Loss: 4.003..  Validation Loss: 4.180..  Early Stopping Patience: 5.. \n","Epoch: 23/30..  Training Loss: 3.991..  Validation Loss: 4.091..  Early Stopping Patience: 4.. \n","Validation loss decreased (4.1003 --> 4.0906)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 24/30..  Training Loss: 3.978..  Validation Loss: 4.123..  Early Stopping Patience: 11.. \n","Epoch: 25/30..  Training Loss: 3.971..  Validation Loss: 4.118..  Early Stopping Patience: 10.. \n","Epoch: 26/30..  Training Loss: 3.950..  Validation Loss: 4.177..  Early Stopping Patience: 9.. \n","Epoch: 27/30..  Training Loss: 3.947..  Validation Loss: 4.091..  Early Stopping Patience: 8.. \n","Validation loss decreased (4.0906 --> 4.0905)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 28/30..  Training Loss: 3.934..  Validation Loss: 4.122..  Early Stopping Patience: 11.. \n","Epoch: 29/30..  Training Loss: 3.920..  Validation Loss: 4.119..  Early Stopping Patience: 10.. \n","Epoch: 30/30..  Training Loss: 3.908..  Validation Loss: 4.101..  Early Stopping Patience: 9.. \n","Done!\n","\n","9 out of 12: Run(epochs=30, lr=0.03, batch_size=32, patience_threshold=10) 0.6869018225097656\n","Epoch: 1/30..  Training Loss: 6.496..  Validation Loss: 2.905..  Early Stopping Patience: 7.. \n","Validation loss decreased (inf --> 2.9047)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 2/30..  Training Loss: 2.860..  Validation Loss: 2.727..  Early Stopping Patience: 7.. \n","Validation loss decreased (2.9047 --> 2.7266)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 3/30..  Training Loss: 2.704..  Validation Loss: 2.634..  Early Stopping Patience: 7.. \n","Validation loss decreased (2.7266 --> 2.6340)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 4/30..  Training Loss: 2.569..  Validation Loss: 2.438..  Early Stopping Patience: 7.. \n","Validation loss decreased (2.6340 --> 2.4383)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 5/30..  Training Loss: 2.491..  Validation Loss: 2.363..  Early Stopping Patience: 7.. \n","Validation loss decreased (2.4383 --> 2.3635)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 6/30..  Training Loss: 2.403..  Validation Loss: 2.473..  Early Stopping Patience: 7.. \n","Epoch: 7/30..  Training Loss: 2.368..  Validation Loss: 2.480..  Early Stopping Patience: 6.. \n","Epoch: 8/30..  Training Loss: 2.335..  Validation Loss: 2.379..  Early Stopping Patience: 5.. \n","Epoch: 9/30..  Training Loss: 2.295..  Validation Loss: 2.363..  Early Stopping Patience: 4.. \n","Validation loss decreased (2.3635 --> 2.3630)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 10/30..  Training Loss: 2.285..  Validation Loss: 2.469..  Early Stopping Patience: 7.. \n","Epoch: 11/30..  Training Loss: 2.247..  Validation Loss: 2.261..  Early Stopping Patience: 6.. \n","Validation loss decreased (2.3630 --> 2.2610)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 12/30..  Training Loss: 2.230..  Validation Loss: 2.204..  Early Stopping Patience: 7.. \n","Validation loss decreased (2.2610 --> 2.2044)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 13/30..  Training Loss: 2.199..  Validation Loss: 2.216..  Early Stopping Patience: 7.. \n","Epoch: 14/30..  Training Loss: 2.187..  Validation Loss: 2.228..  Early Stopping Patience: 6.. \n","Epoch: 15/30..  Training Loss: 2.160..  Validation Loss: 2.203..  Early Stopping Patience: 5.. \n","Validation loss decreased (2.2044 --> 2.2034)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 16/30..  Training Loss: 2.146..  Validation Loss: 2.213..  Early Stopping Patience: 7.. \n","Epoch: 17/30..  Training Loss: 2.122..  Validation Loss: 2.322..  Early Stopping Patience: 6.. \n","Epoch: 18/30..  Training Loss: 2.099..  Validation Loss: 2.312..  Early Stopping Patience: 5.. \n","Epoch: 19/30..  Training Loss: 2.086..  Validation Loss: 2.244..  Early Stopping Patience: 4.. \n","Epoch: 20/30..  Training Loss: 2.063..  Validation Loss: 2.184..  Early Stopping Patience: 3.. \n","Validation loss decreased (2.2034 --> 2.1837)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 21/30..  Training Loss: 2.039..  Validation Loss: 2.214..  Early Stopping Patience: 7.. \n","Epoch: 22/30..  Training Loss: 2.025..  Validation Loss: 2.274..  Early Stopping Patience: 6.. \n","Epoch: 23/30..  Training Loss: 2.009..  Validation Loss: 2.218..  Early Stopping Patience: 5.. \n","Epoch: 24/30..  Training Loss: 1.981..  Validation Loss: 2.182..  Early Stopping Patience: 4.. \n","Validation loss decreased (2.1837 --> 2.1822)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 25/30..  Training Loss: 1.972..  Validation Loss: 2.332..  Early Stopping Patience: 7.. \n","Epoch: 26/30..  Training Loss: 1.951..  Validation Loss: 2.308..  Early Stopping Patience: 6.. \n","Epoch: 27/30..  Training Loss: 1.941..  Validation Loss: 2.215..  Early Stopping Patience: 5.. \n","Epoch: 28/30..  Training Loss: 1.919..  Validation Loss: 2.259..  Early Stopping Patience: 4.. \n","Epoch: 29/30..  Training Loss: 1.910..  Validation Loss: 2.285..  Early Stopping Patience: 3.. \n","Epoch: 30/30..  Training Loss: 1.898..  Validation Loss: 2.256..  Early Stopping Patience: 2.. \n","Done!\n","\n","10 out of 12: Run(epochs=30, lr=0.03, batch_size=64, patience_threshold=6) 0.6869018225097656\n","Epoch: 1/30..  Training Loss: 5.802..  Validation Loss: 3.033..  Early Stopping Patience: 11.. \n","Validation loss decreased (inf --> 3.0327)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 2/30..  Training Loss: 2.888..  Validation Loss: 2.690..  Early Stopping Patience: 11.. \n","Validation loss decreased (3.0327 --> 2.6898)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 3/30..  Training Loss: 2.675..  Validation Loss: 2.604..  Early Stopping Patience: 11.. \n","Validation loss decreased (2.6898 --> 2.6039)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 4/30..  Training Loss: 2.560..  Validation Loss: 2.625..  Early Stopping Patience: 11.. \n","Epoch: 5/30..  Training Loss: 2.475..  Validation Loss: 2.457..  Early Stopping Patience: 10.. \n","Validation loss decreased (2.6039 --> 2.4573)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 6/30..  Training Loss: 2.407..  Validation Loss: 2.393..  Early Stopping Patience: 11.. \n","Validation loss decreased (2.4573 --> 2.3930)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 7/30..  Training Loss: 2.360..  Validation Loss: 2.393..  Early Stopping Patience: 11.. \n","Epoch: 8/30..  Training Loss: 2.332..  Validation Loss: 2.341..  Early Stopping Patience: 10.. \n","Validation loss decreased (2.3930 --> 2.3411)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 9/30..  Training Loss: 2.303..  Validation Loss: 2.364..  Early Stopping Patience: 11.. \n","Epoch: 10/30..  Training Loss: 2.266..  Validation Loss: 2.279..  Early Stopping Patience: 10.. \n","Validation loss decreased (2.3411 --> 2.2791)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 11/30..  Training Loss: 2.247..  Validation Loss: 2.305..  Early Stopping Patience: 11.. \n","Epoch: 12/30..  Training Loss: 2.220..  Validation Loss: 2.388..  Early Stopping Patience: 10.. \n","Epoch: 13/30..  Training Loss: 2.201..  Validation Loss: 2.245..  Early Stopping Patience: 9.. \n","Validation loss decreased (2.2791 --> 2.2446)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 14/30..  Training Loss: 2.161..  Validation Loss: 2.273..  Early Stopping Patience: 11.. \n","Epoch: 15/30..  Training Loss: 2.143..  Validation Loss: 2.305..  Early Stopping Patience: 10.. \n","Epoch: 16/30..  Training Loss: 2.128..  Validation Loss: 2.218..  Early Stopping Patience: 9.. \n","Validation loss decreased (2.2446 --> 2.2184)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 17/30..  Training Loss: 2.111..  Validation Loss: 2.264..  Early Stopping Patience: 11.. \n","Epoch: 18/30..  Training Loss: 2.094..  Validation Loss: 2.204..  Early Stopping Patience: 10.. \n","Validation loss decreased (2.2184 --> 2.2037)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 19/30..  Training Loss: 2.071..  Validation Loss: 2.244..  Early Stopping Patience: 11.. \n","Epoch: 20/30..  Training Loss: 2.064..  Validation Loss: 2.287..  Early Stopping Patience: 10.. \n","Epoch: 21/30..  Training Loss: 2.036..  Validation Loss: 2.256..  Early Stopping Patience: 9.. \n","Epoch: 22/30..  Training Loss: 2.022..  Validation Loss: 2.271..  Early Stopping Patience: 8.. \n","Epoch: 23/30..  Training Loss: 1.998..  Validation Loss: 2.193..  Early Stopping Patience: 7.. \n","Validation loss decreased (2.2037 --> 2.1934)..\n","Saving to  checkpoint.pth \n","\n","Epoch: 24/30..  Training Loss: 1.984..  Validation Loss: 2.224..  Early Stopping Patience: 11.. \n","Epoch: 25/30..  Training Loss: 1.975..  Validation Loss: 2.225..  Early Stopping Patience: 10.. \n","Epoch: 26/30..  Training Loss: 1.960..  Validation Loss: 2.213..  Early Stopping Patience: 9.. \n","Epoch: 27/30..  Training Loss: 1.939..  Validation Loss: 2.267..  Early Stopping Patience: 8.. \n","Epoch: 28/30..  Training Loss: 1.932..  Validation Loss: 2.244..  Early Stopping Patience: 7.. \n","Epoch: 29/30..  Training Loss: 1.916..  Validation Loss: 2.275..  Early Stopping Patience: 6.. \n","Epoch: 30/30..  Training Loss: 1.901..  Validation Loss: 2.206..  Early Stopping Patience: 5.. \n","Done!\n","\n","11 out of 12: Run(epochs=30, lr=0.03, batch_size=64, patience_threshold=10) 0.6869018225097656\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"76RZGuuyR_XC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609194374149,"user_tz":-120,"elapsed":7896812,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"}},"outputId":"d9e49401-97f2-4f78-ff97-7460f9aa0851"},"source":["test_loader = torch.utils.data.DataLoader(test_set, batch_size = best_run.batch_size, num_workers = 0)\n","test(best_model, test_loader, criterion)"],"execution_count":40,"outputs":[{"output_type":"stream","text":["Test Loss: 0.7913\n","Test Accuracy of airplane: 73% (736/1000)\n","Test Accuracy of automobile: 93% (939/1000)\n","Test Accuracy of  bird: 73% (730/1000)\n","Test Accuracy of   cat: 68% (685/1000)\n","Test Accuracy of  deer: 81% (819/1000)\n","Test Accuracy of   dog: 72% (721/1000)\n","Test Accuracy of  frog: 88% (889/1000)\n","Test Accuracy of horse: 80% (802/1000)\n","Test Accuracy of  ship: 89% (897/1000)\n","Test Accuracy of truck: 85% (856/1000)\n","\n","Test Accuracy (Overall): 80% (8074/10000)\n"],"name":"stdout"}]}]}