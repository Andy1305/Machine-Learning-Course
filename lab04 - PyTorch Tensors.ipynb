{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lab04 - PyTorch Tensors.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"}},"cells":[{"cell_type":"markdown","metadata":{"id":"V5aI3m_T32pR"},"source":["# Laboratory 04: PyTorch and Tensors Operations\n","\n","In this laboratory you'll get introduced to [PyTorch](http://pytorch.org/), a framework for building and training neural networks. Specifically, today we'll explore operations that can be applied to tensors using this framework. Understanding how tensors work is essential for both building, training and inspecting different aspects of a neural network. \n","\n","As you'll see, PyTorch, in a lot of ways behaves like the arrays you may love from Numpy. These Numpy arrays, after all, are just tensors. PyTorch takes these tensors and makes it simple to move them to GPUs for the faster processing needed when training neural networks. \n","\n","The PyTorch framework main characteristics are:\n"," - a thin framework over python that inherits its core from its Lua predecessor called Torch\n"," - dynamically generates neural network computational graphs\n"," - it's object oriented with powerful debugging support\n","\n","which adhere to a development philosophy that promotes linear code-flow, integrates full inter-operability with Python ecosystem and is as fast as other frameworks like TensorFlow, Keras or CNTK.\n"]},{"cell_type":"code","metadata":{"id":"4TucqG1132pW","executionInfo":{"status":"ok","timestamp":1605431349590,"user_tz":-120,"elapsed":1247,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"}}},"source":["import numpy as np"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1zx-ZARd32pc"},"source":["## PyTorch Main Components\n","\n","At the top level, the PyTorch package and tensor library is simply called [`torch`](https://pytorch.org/docs/stable/torch.html). While the features that allow us to build and train state of the art networks are split among:\n","-  [`torch.nn`](https://pytorch.org/docs/stable/nn.html) a subpackage that contains modules and extensible classes for building neural networks.\n","- [`torch.autograd`](https://pytorch.org/docs/stable/autograd.html?highlight=autograd#) a subpackage that supports all the differentiable tensor operations.\n","- [`torch.nn.functional`](https://pytorch.org/docs/stable/nn.functional.html#torch-nn-functional) a functional interface that contains typical operations used for building neural networks like loss functions, activation functions and convolution operations.\n","\n","- [`torch.optim`](https://pytorch.org/docs/stable/optim.html#module-torch.optim) a subpackage that contains standard optimization operations such as SGD, Adam and so on.\n","\n","- [`torch.utils`](https://pytorch.org/docs/stable/data.html) a subpackage that contains utility classes like data sets and data loaders that make data preprocessing easier.\n","\n","Aside from these components the framework also includes packages such as [`torchvision`](https://pytorch.org/docs/stable/torchvision/index.html), [`torchaudio`](https://pytorch.org/audio/) and [`torchtext`](https://pytorch.org/text/) that provide access to popular datasets, neural networks architectures and transformations for computer vision, audio and natural language processing.\n","\n","## PyTorch Tensors\n","\n","It turns out neural network computations are just a bunch of linear algebra operations using the *tensor* generalization. Recall that a vector is a 1-dimensional tensor, a matrix is a 2-dimensional tensor, an array with three indices is a 3-dimensional tensor (RGB color images for example). The fundamental data structure for neural networks are tensors and PyTorch (as well as pretty much every other deep learning framework) is built around tensors.\n","\n","<img src=\"res/tensor_examples.svg\" width=600px>\n","\n","In PyTorch tensors are instances of the [`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html) class and each tensor has the following attributes:\n","- torch.dtype is the data type of the tensor component elements.\n","- torch.device represents the device where the data is allocated CPU / GPU.\n","- torch.layout describes how a tensor is mapped in memory.\n","- torch.shape which, as the name implies is the shape of the tensor.\n","\n","Note that, since a tensor can be allocated either on the CPU or GPU, the data type of each element composing a tensor depends both on `dtype` and `device` attributes. The table below summaries data types supported by PyTorch tensors:\n","\n","| Data Type  | dtype  | CPU tensor  | GPU tensor  |\n","|---|---|---|---|\n","| 32-bit floating point  | torch.float32  | torch.FloatTensor  | torch.cuda.FloatTensor  |\n","| 64-bit floating point  | torch.float64  | torch.DoubleTensor  | torch.cuda.DoubleTensor  |\n","| 16-bit floating point  | torch.float16  | torch.HalfTensor  | torch.cuda.HalfTensor  |\n","| 8-bit integer (unsigned)  | torch.uint8  | torch.ByteTensor  | torch.cuda.ByteTensor  |\n","| 8-bit integer (signed)  | torch.int8  | torch.CharTensor  | torch.cuda.CharTensor  |\n","| 16-bit integer (signed)  | torch.int16  | torch.ShortTensor   | torch.cuda.ShortTensor  |\n","| 32-bit integer (signed)  | torch.int32  | torch.IntTensor  | torch.cuda.IntTensor  |\n","| 64-bit integer (signed)  | torch.int64  | torch.LongTensor  | torch.cuda.LongTensor  |\n","\n","**Exercise 1**\n","\n","Ok, now lets get down to business by first importing the `torch` package."]},{"cell_type":"code","metadata":{"id":"JMGQT7JR32pd","executionInfo":{"status":"ok","timestamp":1605431349587,"user_tz":-120,"elapsed":3960,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"}}},"source":["# TODO 1.1. Import the torch package\n","import torch"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xQ135m4232ph"},"source":["### Creating A Tensor\n","\n","In PyTorch there are four options to create a tensor, namely:\n","\n","1. Using the constructor `torch.Tensor(data)`\n","2. Using the factory method `torch.tensor(data)`\n","3. Using the the multiple input class method `torch.as_tensor(data)`\n","4. Using the conversion from numpy `torch.from_numpy(data)`.\n","\n","A couple of things to note here are: i) the constructor uses the default torch.float32 dtype without performing any inference based on input type and ii) both `as_tensor(data)` and `from_numpy(data)` methods share the memory with the input data structure, i.e. changing the elements of one will change the other.\n","\n","We can see this by creating two identical tensors form the same list of numbers."]},{"cell_type":"code","metadata":{"id":"lFNsNR0P32ph","executionInfo":{"elapsed":4403,"status":"ok","timestamp":1605354540994,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"},"user_tz":-120},"outputId":"f0bc0d34-3113-4a60-caa2-390eb743dfe7","colab":{"base_uri":"https://localhost:8080/"}},"source":["data = [[1,2],[3,4]]\n","\n","# TODO 1.2. Create a tensor t1 using the constructor & \"data\"\n","t1 = torch.Tensor(data)\n","print(t1)\n","\n","# TODO 1.3. Print its dtype attribute\n","print(t1.dtype)\n","\n","# TODO 1.4. Similarly create t2, but using the factory method\n","t2 = torch.tensor(data)\n","print(t2)\n","print(t2.dtype)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[1., 2.],\n","        [3., 4.]])\n","torch.float32\n","tensor([[1, 2],\n","        [3, 4]])\n","torch.int64\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"t6rkNjYH32pl"},"source":["Now lets see what happens if we create a tensor from a numpy array and then change the array elements."]},{"cell_type":"code","metadata":{"id":"Qnqqi_Sb32pm","executionInfo":{"elapsed":4394,"status":"ok","timestamp":1605354540995,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"},"user_tz":-120},"outputId":"38a03cff-a1b4-4d86-bc3a-264b1d6e4b59","colab":{"base_uri":"https://localhost:8080/"}},"source":["data = np.array([[1, 2],[3, 4]])\n","\n","# TODO 1.5. Use the numpy conversion method to create t\n","t = torch.from_numpy(data)\n","\n","# TODO: 1.6. Print the tensor t\n","print(t)\n","\n","# TODO: 1.7. Change element at index (0, 0) of data\n","data[0, 0] = 13\n","\n","# TODO: 1.8. Print the tensor t again\n","print(t)\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[1, 2],\n","        [3, 4]])\n","tensor([[13,  2],\n","        [ 3,  4]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VL8Z2JCs32pp"},"source":["Of course, similarly to numpy we also have at our disposal methods such as `rand()`,`zeros()`, `one()`, `eye()` and so on. For example, creating a rank 1 tensor of 3 random numbers ca be accomplished trough:\n","\n","```Python\n","t = torch.rand(3, 1)\n","```\n","\n","Now, to exercise this yourself, please create a rank 2 tensor of all ones in the TODO below. (see [.ones()](https://pytorch.org/docs/stable/generated/torch.ones.html?highlight=ones#torch.ones))"]},{"cell_type":"code","metadata":{"id":"ta3z39ag32pq","executionInfo":{"elapsed":4387,"status":"ok","timestamp":1605354540996,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"},"user_tz":-120},"outputId":"73844bbc-2e65-40ba-86f9-7f4893d44867","colab":{"base_uri":"https://localhost:8080/"}},"source":["# TODO 1.9. Create a rank two tensor and print it\n","t = torch.ones(3, 2)\n","print(t)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[1., 1.],\n","        [1., 1.],\n","        [1., 1.]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nzCLpDqv32pt"},"source":["### Tensor Shape\n","\n","Suppose that we have the following tensor:\n","$$\n","t = \\begin{bmatrix}\n","1 &1 &1 &1 \\\\ \n","2 &2 &2 &2 \\\\ \n","3 &3 &3 &3 \n","\\end{bmatrix}\n","$$\n","\n","The shape of this tensor is 3 x 4, with a rank of 2. Remember, *rank* means the number of dimensions present within a tensor. \n","\n","**Exercise 2**\n","\n","Now lets create this tensor and play around with it a bit."]},{"cell_type":"code","metadata":{"id":"GUEJVjqq32pu","executionInfo":{"status":"ok","timestamp":1605431358375,"user_tz":-120,"elapsed":1161,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"}},"outputId":"546e9f55-b68e-4128-93b2-23517d4a7c03","colab":{"base_uri":"https://localhost:8080/"}},"source":["# TODO 2.1. Create the tensor t from the example\n","t = torch.tensor([[1] * 4, [2] * 4, [3] * 4])\n","print(t)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["tensor([[1, 1, 1, 1],\n","        [2, 2, 2, 2],\n","        [3, 3, 3, 3]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"07g-MqHi32py"},"source":["PyTorch provides us with two ways to find the shape of a tensor:\n","- via the t.shape attribute\n","- via the t.size() method\n","\n","Both methods return a `torch.Size()` object. Let's see this in action."]},{"cell_type":"code","metadata":{"id":"Oeilos0E32pz","executionInfo":{"elapsed":4373,"status":"ok","timestamp":1605354540997,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"},"user_tz":-120},"outputId":"a286ecd2-9d8e-43a5-af48-317670189f19","colab":{"base_uri":"https://localhost:8080/"}},"source":["# TODO 2.2: Print the shape of the tensor\n","print(t.shape) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([3, 4])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"f1UZXHJj32p3"},"source":["As mentioned in the course the length of the shape of a tensor is just a way of counting the number of dimensions in a tensor. Hence, the length of the shape object represents the rank of the tensor."]},{"cell_type":"code","metadata":{"id":"ZPvSPkCL32p3","executionInfo":{"elapsed":4367,"status":"ok","timestamp":1605354540998,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"},"user_tz":-120},"outputId":"69c39e29-92d8-4a82-feba-5fca43dbb2a3","colab":{"base_uri":"https://localhost:8080/"}},"source":["# TODO 2.3. Print the rank of the tensor\n","len(t.shape)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"QK911UKi32p6"},"source":["We can also compute the number of individual elements in a tensor. It simply represents the product of the shape component values. Hence we can do something like:\n","\n","```Python\n","torch.tensor(t.shape).prod()\n","```\n","However, the resulting number not be an integer. It will be a tensor that contains our integer. To obtain it we can directly use the [`.numel()`](https://pytorch.org/docs/stable/generated/torch.numel.html?highlight=numel#torch.numel) method on our tensor."]},{"cell_type":"code","metadata":{"id":"9DDfD3Eg32p7","executionInfo":{"elapsed":4360,"status":"ok","timestamp":1605354540999,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"},"user_tz":-120},"outputId":"b85f1b06-434d-4724-f2e7-0bb7e1971a9f","colab":{"base_uri":"https://localhost:8080/"}},"source":["# TODO 2.4. Print the number of elments in our tensor\n","t.numel()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["12"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"V4Y7H7D632p-"},"source":["### Tensor Reshaping\n","\n","Reshaping is the most frequently used operation and it is required to format tensors as they pass from the output of one  neural network layer type to the input of a different type of neural network layer. For example, when passing from a convolution layer to a fully connected layer. More on thins latter in the course. The operation is lossless in the sense that we don't change, remove or add any new data entries. As the name entails, the reshaping operation only changes how the data entries are organized within the tensor.\n","\n","The simplest type of reshaping is the one that does not change the tensor rank. In PyTorch this can be accomplished via the [`.reshape()`](https://pytorch.org/docs/stable/tensors.html?highlight=reshape#torch.Tensor.reshape) or [`.view()`](https://pytorch.org/docs/stable/tensors.html?highlight=reshape#torch.Tensor.view) methods. For example, we could reshape our previous tensor such as it looks more like a vector than a matrix:"]},{"cell_type":"code","metadata":{"id":"eygVdaUZ32p_","executionInfo":{"elapsed":4353,"status":"ok","timestamp":1605354540999,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"},"user_tz":-120},"outputId":"5e76ee82-8dd0-4340-da4a-6b1c55ba3893","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Original tensor and its shape\n","print(t)\n","print(t.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[1, 1, 1, 1],\n","        [2, 2, 2, 2],\n","        [3, 3, 3, 3]])\n","torch.Size([3, 4])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TLq1ZpbF32qC","executionInfo":{"elapsed":5266,"status":"ok","timestamp":1605354541919,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"},"user_tz":-120},"outputId":"188228a1-de4c-4f37-ad1e-8c99a8607759","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Reshaping to 1 row and 12 columns\n","new_t = t.reshape(1, 12)\n","\n","# Reshaped tensor and its shape\n","print(new_t)\n","print(new_t.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3]])\n","torch.Size([1, 12])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xs02IWlO32qG"},"source":["Note, that although the new tensor looks more like vector it still has rank 2. That is, it still has two axes: the first with a length of 1, while the second with a length of 12. Also note, that in reshaping the new axes lengths we have passed yield a total of 12 data entries. If we were to pass incompatible axes lengths with respect to the total number of entries, we would get an error similar to the one below:"]},{"cell_type":"code","metadata":{"id":"FU4wt4kc32qH","executionInfo":{"elapsed":5292,"status":"error","timestamp":1605354541953,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"},"user_tz":-120},"outputId":"0bd0004d-7594-4f23-abb7-d558ec63e6d7","colab":{"base_uri":"https://localhost:8080/","height":163}},"source":["t.reshape(5,6)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-1ab35077fa08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m: shape '[5, 6]' is invalid for input of size 12"]}]},{"cell_type":"markdown","metadata":{"id":"dpsRIsWh32qK"},"source":["A neat trick we can use when doing reshapes like the one above is to let the `.reshape()` / `.view()` methods decide which is the correct length for one of the axes. For example, the previous reshape could have been written as:"]},{"cell_type":"code","metadata":{"id":"xakuemSB32qL","executionInfo":{"elapsed":1120,"status":"ok","timestamp":1605354550079,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"},"user_tz":-120},"outputId":"2c5beeb6-254b-4c6b-bb6e-338c77ac905f","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Reshaping to 1 row and 12 columns\n","new_t = t.reshape(1, -1)\n","\n","# Reshaped tensor and its shape\n","print(new_t)\n","print(new_t.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3]])\n","torch.Size([1, 12])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pFQsl5jZ32qO"},"source":["While here we have used `.reshape()`, you can used `.view()` in the same manner to achieve the same result. The difference is that `.view()` always clones the data (makes a copy of it) to build a tensor of the specified shape, whereas `.reshape()` will try not to copy the data if its possible."]},{"cell_type":"markdown","metadata":{"id":"Pw0keWay32qP"},"source":["**Exercise 3**\n","\n","Now its your turn to play around with reshaping. So, repeat the steps above for the following sizes \\[2, 6\\], \\[3, 4\\], \\[6, 2\\], \\[12, 1\\], \\[2, -1 \\], \\[-1, 1\\]."]},{"cell_type":"code","metadata":{"id":"4IU24FII32qQ","executionInfo":{"elapsed":1135,"status":"ok","timestamp":1605354554331,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"},"user_tz":-120},"outputId":"8ea9fa53-1052-4aa7-cb1a-e08883a8da8d","colab":{"base_uri":"https://localhost:8080/"}},"source":["# TODO 3.1. Reshape and print each reshaped tensor\n","print(t.reshape(2, 6))\n","print(t.reshape(3, 4))\n","print(t.reshape(6, 2))\n","print(t.reshape(12, 1))\n","print(t.reshape(2, -1))\n","print(t.reshape(-1, 1))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[1, 1, 1, 1, 2, 2],\n","        [2, 2, 3, 3, 3, 3]])\n","tensor([[1, 1, 1, 1],\n","        [2, 2, 2, 2],\n","        [3, 3, 3, 3]])\n","tensor([[1, 1],\n","        [1, 1],\n","        [2, 2],\n","        [2, 2],\n","        [3, 3],\n","        [3, 3]])\n","tensor([[1],\n","        [1],\n","        [1],\n","        [1],\n","        [2],\n","        [2],\n","        [2],\n","        [2],\n","        [3],\n","        [3],\n","        [3],\n","        [3]])\n","tensor([[1, 1, 1, 1, 2, 2],\n","        [2, 2, 3, 3, 3, 3]])\n","tensor([[1],\n","        [1],\n","        [1],\n","        [1],\n","        [2],\n","        [2],\n","        [2],\n","        [2],\n","        [3],\n","        [3],\n","        [3],\n","        [3]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PF-i2m8-32qT"},"source":["Of course, we can also do reshapes that do change rank. That is, we can introduce a new axis or remove an existing one. As long as we preserve the number of data entries in the tensor the reshape will work as expected. To see this complete the TODOs below:"]},{"cell_type":"code","metadata":{"id":"N2yniHD932qU","executionInfo":{"elapsed":1159,"status":"ok","timestamp":1605354558522,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"},"user_tz":-120},"outputId":"c3b91699-266c-4246-afa6-b5a1810d8d62","colab":{"base_uri":"https://localhost:8080/"}},"source":["# TODO 3.2. Reshape tensor to a shape of [2,2,3]\n","new_t = t.reshape(2, 2, 3)\n","\n","# TODO 3.3. Print the new tensor\n","print(new_t)\n","\n","# TODO 3.4. Print the new tensor shape\n","print(new_t.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[[1, 1, 1],\n","         [1, 2, 2]],\n","\n","        [[2, 2, 3],\n","         [3, 3, 3]]])\n","torch.Size([2, 2, 3])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fXqmOiTY32qY","executionInfo":{"elapsed":1183,"status":"ok","timestamp":1605354561249,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"},"user_tz":-120},"outputId":"57d81a47-287f-467c-8cc9-30447a6633cb","colab":{"base_uri":"https://localhost:8080/"}},"source":["# TODO 3.5. Reshape tensor to a shape of [2,-1,3]\n","new_t = t.reshape(2, -1, 3)\n","\n","# TODO 3.6. Print the new tensor\n","print(new_t)\n","\n","# TODO 3.7. Print the new tensor shape\n","print(new_t.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[[1, 1, 1],\n","         [1, 2, 2]],\n","\n","        [[2, 2, 3],\n","         [3, 3, 3]]])\n","torch.Size([2, 2, 3])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ju8tCxf732qc"},"source":["We can also remove or add axes by using [`.squeeze()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.squeeze) or [`.unsqueeze()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.unsqueeze), respectively. That is, *squeezing* a tensor removes the dimensions or axes that have a length of 1, whereas *unsqueezing* a tensor adds a dimension with a length of 1. Hence this methods allow us to modify the rank, either by expanding or shrinking the tensor in question. Note, that the `.unsqueeze()` method requires us to specify on what axis is the new dimension added via the `dim=` parameter. We can of course specify whatever axis we want. \n","\n","To see how `.squeeze()` works complete the TODO's below."]},{"cell_type":"code","metadata":{"id":"J73NXUaR32qc","executionInfo":{"status":"ok","timestamp":1605431368279,"user_tz":-120,"elapsed":1155,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"}},"outputId":"f394ec63-9a5e-433d-8477-64016a551de7","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Reshaping the tensor\n","new_t = t.reshape(1,-1)\n","\n","# TODO 3.8. Print the new tensor shape\n","print(new_t)\n","print(new_t.shape)\n","\n","# TODO 3.9. Squeeze the new_t and print its shape\n","new_t = new_t.squeeze()\n","print(new_t)\n","print(new_t.shape)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["tensor([[1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3]])\n","torch.Size([1, 12])\n","tensor([1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3])\n","torch.Size([12])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MIFb8FYT32qf"},"source":["Now do the opposite operation on the new tensor via [`.unsqueeze()`](https://pytorch.org/docs/stable/tensors.html?highlight=unsqueeze#torch.Tensor.unsqueeze) to add back its length 1 axis on the appropriate dimension. Note, that the dimension indexes start at 0."]},{"cell_type":"code","metadata":{"id":"Le2rUZc132qg","executionInfo":{"elapsed":5221,"status":"aborted","timestamp":1605354541923,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"},"user_tz":-120},"outputId":"f7b74308-50c3-47c7-9b4a-2d3028e1fc26","colab":{"background_save":true}},"source":["# TODO 3.10. Unsqueeze the new_t and print its shape\n","new_t = new_t.unsqueeze(dim = 0)\n","print(new_t)\n","print(new_t.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3]])\n","torch.Size([1, 12])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gb1aH8qi32qi"},"source":["Ok, so we can reshape a tensor and add/remove length 1 axes more easily via squeezing and unsqueezing. However, there is one reshape operation that is so intensely used within the forward flow of tensors through a neural network that we have dedicated method for it, namely: [`.flatten()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.flatten) for the so called *flattening operation*.\n","\n","The flattening operation simply reshapes a tensor such that data entries in the tensor are mapped in a sequence on a single axis. Without any arguments method returns a rank 1 tensor that resembles a 1D array of numbers. Nonetheless, this method provides us with some flexibility that allows us to keep the first few axes intact while flattening the rest via the `.start_dim=` argument. As its name implies the flattening operation occurs starting on the specified dimension.\n","\n","Complete the TODO's below."]},{"cell_type":"code","metadata":{"id":"aG1PXSTg32qj","executionInfo":{"status":"ok","timestamp":1605357438654,"user_tz":-120,"elapsed":575,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"}},"outputId":"047b53fc-1c32-4d03-987b-3a6cba935e8a","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Suppose we have the following tensor\n","t = torch.tensor([\n","        [[1, 1, 1, 1],\n","         [1, 1, 1, 1],\n","         [1, 1, 1, 1],\n","         [1, 1, 1, 1]],\n","\n","        [[2, 2, 2, 2],\n","         [2, 2, 2, 2],\n","         [2, 2, 2, 2],\n","         [2, 2, 2, 2]],\n","\n","        [[3, 3, 3, 3],\n","         [3, 3, 3, 3],\n","         [3, 3, 3, 3],\n","         [3, 3, 3, 3]]])\n","\n","# TODO 3.11. Print the tensor shape\n","print(t.shape)\n","\n","# TODO 3.12. Flatten the entire tensor and save it in new_t \n","new_t = t.flatten()\n","\n","# TODO 3.13. Print new_t tensor and its shape\n","print(new_t)\n","print(new_t.shape)\n","\n","# TODO 3.14. Flatten t staring on dimension one\n","new_t = t.flatten(start_dim = 1)\n","\n","# TODO 3.13. Print new_t tensor and its shape\n","print(new_t)\n","print(new_t.shape)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([3, 4, 4])\n","tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n","        2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])\n","torch.Size([48])\n","tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","        [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n","        [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]])\n","torch.Size([3, 16])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"m0bdlSAg32qm"},"source":["#### PyTorch Image Tensor Example\n","\n","When dealing with neural networks that classify images, we need a tensor representation. In PyTorch this representation is given by a rank 4 tensor with the format \\[B, C, H, W \\] where:\n","\n","- B represents the batch size, i.e. number of images in a tensor\n","- C represents the number of channels in an image, e.g. monochrome images have $C=1$, whereas color images usually have $C=3$ for the RGB color system. \n","- H represents the image height in the number of pixels\n","- W represents the image width also in the number of pixels\n","\n","Now suppose we have a tensor such as the one given bellow:"]},{"cell_type":"code","metadata":{"id":"Bqwn7Ftw32qm","executionInfo":{"status":"ok","timestamp":1605357641286,"user_tz":-120,"elapsed":608,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"}},"outputId":"750d8ca8-7571-4297-f1bc-fadd6e8c029a","colab":{"base_uri":"https://localhost:8080/"}},"source":["# an image batch tensor\n","a_image_batch = torch.tensor([\n","        [[[1, 1, 1, 1],\n","         [1, 1, 1, 1],\n","         [1, 1, 1, 1],\n","         [1, 1, 1, 1]],\n","\n","        [[2, 2, 2, 2],\n","         [2, 2, 2, 2],\n","         [2, 2, 2, 2],\n","         [2, 2, 2, 2]],\n","\n","        [[3, 3, 3, 3],\n","         [3, 3, 3, 3],\n","         [3, 3, 3, 3],\n","         [3, 3, 3, 3]]]])\n","\n","print(a_image_batch)\n","print(a_image_batch.shape)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[[[1, 1, 1, 1],\n","          [1, 1, 1, 1],\n","          [1, 1, 1, 1],\n","          [1, 1, 1, 1]],\n","\n","         [[2, 2, 2, 2],\n","          [2, 2, 2, 2],\n","          [2, 2, 2, 2],\n","          [2, 2, 2, 2]],\n","\n","         [[3, 3, 3, 3],\n","          [3, 3, 3, 3],\n","          [3, 3, 3, 3],\n","          [3, 3, 3, 3]]]])\n","torch.Size([1, 3, 4, 4])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"r3BKC7I732qq"},"source":["This tensor is a batch containing only 1 image, that has 3 channels and the image size is 4x4 pixels in height and width. When we flattening this tensor we usually want to keep the batch information intact since each image is different, but flatten the rest. So flatten the `a_image_batch` tensor by keeping the batch size intact in the TODO's bellow."]},{"cell_type":"code","metadata":{"id":"ChDSHvwa32qq","executionInfo":{"status":"ok","timestamp":1605358019223,"user_tz":-120,"elapsed":584,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"}},"outputId":"65711785-8027-4b2f-8910-73faae32c7e2","colab":{"base_uri":"https://localhost:8080/"}},"source":["# TODO 3.14. Flatten a_image_batch via .flatten() and print it and its shape\n","t_flatten = a_image_batch.flatten(start_dim = 1)\n","print(t_flatten)\n","print(t_flatten.shape)\n","\n","# TODO 3.15. Flatten a_image_batch via .reshape() and print it and its shape\n","t_reshape = a_image_batch.reshape(1, -1)\n","print(t_reshape)\n","print(t_reshape.shape)\n","\n","# TODO 3.15. Flatten a_image_batch via .view() and print it and its shape\n","t_view = a_image_batch.view(1, -1)\n","print(t_view)\n","print(t_view.shape)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n","         2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]])\n","torch.Size([1, 48])\n","tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n","         2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]])\n","torch.Size([1, 48])\n","tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n","         2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]])\n","torch.Size([1, 48])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QAunfWrb32qu"},"source":["#### Concatenating PyTorch Tensors\n","\n","Similarly to Pandas, in Pytorch we can concatenate tensors based on a specific axis via the [`torch.cat()`](https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat) method. The method accepts a tuple of tensors (e.g. `(t1, t2)`) and the dimension `dim=` on which to concatenate them. Note that the tensors which we want to concatenate must be of the same shape (except the dimension we are concatenating on)  or empty.\n","\n","Let's spouse we have the following two tensors:"]},{"cell_type":"code","metadata":{"id":"jDSWgbQR32qu","executionInfo":{"status":"ok","timestamp":1605358142116,"user_tz":-120,"elapsed":623,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"}},"outputId":"3eb2bfce-1e9d-471f-a3f1-2e853791b4a1","colab":{"base_uri":"https://localhost:8080/"}},"source":["t1 = torch.tensor([\n","    [1,2],\n","    [3,4]\n","])\n","\n","t2 = torch.tensor([\n","    [5,6],\n","    [7,8]\n","])\n","\n","print('t1 shape: ', t1.shape)\n","print('t2 shape: ', t2.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["t1 shape:  torch.Size([2, 2])\n","t2 shape:  torch.Size([2, 2])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zPdtSPTx32qy"},"source":["We can concatenate row-wise (axis at index 0) in following way:"]},{"cell_type":"code","metadata":{"id":"VTEpAP-U32qy","executionInfo":{"status":"ok","timestamp":1605358208768,"user_tz":-120,"elapsed":511,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"}},"outputId":"53ff266f-ddc5-469d-c107-58e58cdbcdd9","colab":{"base_uri":"https://localhost:8080/"}},"source":["t = torch.cat((t1, t2), dim=0)\n","\n","print(t)\n","print(t.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[1, 2],\n","        [3, 4],\n","        [5, 6],\n","        [7, 8]])\n","torch.Size([4, 2])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"otvbeS3H32q2"},"source":["As can be observed the result joins the two tensors based on the first axis, i.e. row wise. Now repeat the process such that the concatenation occurs column-wise and print the result."]},{"cell_type":"code","metadata":{"id":"OpWJWOMP32q2","executionInfo":{"status":"ok","timestamp":1605358259216,"user_tz":-120,"elapsed":891,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"}},"outputId":"9601dabf-8ef0-4f90-ae3e-e219276bb3c4","colab":{"base_uri":"https://localhost:8080/"}},"source":["# TODO 3.16. Concatenate t1, t2 column wise\n","t = torch.cat((t1, t2), dim = 1)\n","\n","# TODO 3.17. Print the result and its shape\n","print(t)\n","print(t.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[1, 2, 5, 6],\n","        [3, 4, 7, 8]])\n","torch.Size([2, 4])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6hIvlInh32q5"},"source":["#### Stacking PyTorch Tensors\n","\n","Aside from combining tensor via concatenation, we may wish to combine tensors on a new axis (e.g. to create a batch out of separate images). This is called stacking and is accomplished via the [`stack()`](https://pytorch.org/docs/stable/generated/torch.stack.html#torch.stack) method. For example, suppose we have the following separate monocrom 4x4 images bellow:"]},{"cell_type":"code","metadata":{"id":"ICw4uWfr32q6","executionInfo":{"status":"ok","timestamp":1605358299375,"user_tz":-120,"elapsed":1102,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"}},"outputId":"b24b1d91-c483-4b2f-bc5b-b71963f27376","colab":{"base_uri":"https://localhost:8080/"}},"source":["t1 = torch.ones(4,4)\n","t2 = 2*torch.ones(4,4)\n","t3 = 3*torch.ones(4,4)\n","\n","print('t1 shape: ', t1.shape)\n","print('t2 shape: ', t2.shape)\n","print('t3 shape: ', t3.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["t1 shape:  torch.Size([4, 4])\n","t2 shape:  torch.Size([4, 4])\n","t3 shape:  torch.Size([4, 4])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MMih6DwC32q8"},"source":["To create an image batch of shape \\[3, 1, 4, 4\\] we can use the `.stack()` in combination with `.unsqueeze()` on the second axis in the TODO's below."]},{"cell_type":"code","metadata":{"id":"5Fo-4faE32q9","executionInfo":{"status":"ok","timestamp":1605358574046,"user_tz":-120,"elapsed":805,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"}},"outputId":"072814bd-64c3-449b-9263-68c80c8ee7dd","colab":{"base_uri":"https://localhost:8080/"}},"source":["# TODO 3.18. Create a [3, 1, 4, 4] image batch using t1, t2, t3\n","t = torch.stack([t1, t2, t3], dim = 0)\n","t = t.unsqueeze(dim = 1)\n","\n","# TODO 3.19. Print the result and its shape\n","print(t)\n","print(t.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[[[1., 1., 1., 1.],\n","          [1., 1., 1., 1.],\n","          [1., 1., 1., 1.],\n","          [1., 1., 1., 1.]]],\n","\n","\n","        [[[2., 2., 2., 2.],\n","          [2., 2., 2., 2.],\n","          [2., 2., 2., 2.],\n","          [2., 2., 2., 2.]]],\n","\n","\n","        [[[3., 3., 3., 3.],\n","          [3., 3., 3., 3.],\n","          [3., 3., 3., 3.],\n","          [3., 3., 3., 3.]]]])\n","torch.Size([3, 1, 4, 4])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Svyn9WU132q_"},"source":["### Tensor Element-wise Operations\n","\n","With element-wise operations we do all kinds of arithmetic and comparison operations between tensors. An element-wise operation is an operation between two tensors, of the same shape, that operates on corresponding elements within the respective tensors. Here, two elements are said to be corresponding if they occupy the same position within tensors. The position is determined by the indexes used to locate each element. \n","\n","#### Arithmetic Operations\n","\n","Tensors support all kinds of arithmetic operations like addition, subtraction, multiplication or division. In PyTorch these are operations supported through `.add()`, `.sub()`, `.mul()` and `.div()` methods and there overwritten operators `+`, `-`, `*` and `/`. Suppose we have the following tensors:"]},{"cell_type":"code","metadata":{"id":"5aB9Ae8v32rA"},"source":["t1 = torch.tensor([\n","    [1,1],\n","    [1,1]\n","], dtype=torch.float32)\n","\n","t2 = torch.tensor([\n","    [1,2],\n","    [3,4]\n","], dtype=torch.float32)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"03Ud84Pe32rC"},"source":["To add t1 and t2 we can use either the method or the operator as in the example below."]},{"cell_type":"code","metadata":{"id":"bt1mxm5k32rD","executionInfo":{"status":"ok","timestamp":1605358726900,"user_tz":-120,"elapsed":759,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"}},"outputId":"f06e5595-db0e-4f47-bcad-5df59281acb9","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Adding via the operator\n","t = t1 + t2\n","print(t)\n","\n","# Adding via the method\n","t = t1.add(t2)\n","print(t)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[2., 3.],\n","        [4., 5.]])\n","tensor([[2., 3.],\n","        [4., 5.]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qYgG2Ux432rG"},"source":["**Exercise 4**\n","\n","Now, proceed to subtract, multiply and divide the two tensors via operators and corresponding methods in the TODO's below. As in the example above print out the result after each operation. "]},{"cell_type":"code","metadata":{"id":"ViFIDAsW32rG","executionInfo":{"status":"ok","timestamp":1605358834985,"user_tz":-120,"elapsed":969,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"}},"outputId":"fcefbfbc-65fb-451f-b440-50fc1d6610ae","colab":{"base_uri":"https://localhost:8080/"}},"source":["# TODO 4.1. Subtract the two tensors\n","t = t1 - t2\n","print(t)\n","t = t1.sub(t2)\n","print(t)\n","# TODO 4.2. Multiply the two tensors\n","t = t1 * t2\n","print(t)\n","t = t1.mul(t2)\n","print(t)\n","# TODO 4.3. Divide the two tensors\n","t = t1 / t2\n","print(t)\n","t = t1.div(t2)\n","print(t)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[ 0., -1.],\n","        [-2., -3.]])\n","tensor([[ 0., -1.],\n","        [-2., -3.]])\n","tensor([[1., 2.],\n","        [3., 4.]])\n","tensor([[1., 2.],\n","        [3., 4.]])\n","tensor([[1.0000, 0.5000],\n","        [0.3333, 0.2500]])\n","tensor([[1.0000, 0.5000],\n","        [0.3333, 0.2500]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"K_YRS6NK32rJ"},"source":["We can also do the same operations between tensors and scalars. Complete the TODO's below to see how this works.\n","\n","> Indented block\n","\n"]},{"cell_type":"code","metadata":{"id":"K7kPwavH32rJ","executionInfo":{"status":"ok","timestamp":1605359065943,"user_tz":-120,"elapsed":563,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"}},"outputId":"0bc15980-adc7-44d6-efae-8a9d5eed9fcb","colab":{"base_uri":"https://localhost:8080/"}},"source":["# TODO 4.4. Add 2 to the t1 tensor and print the result\n","add_2 = t1 + 2\n","print(add_2)\n","\n","# TODO 4.5. Divide by 2 the t1 tensor and print the result\n","div_2 = t1 / 2\n","print(div_2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[3., 3.],\n","        [3., 3.]])\n","tensor([[0.5000, 0.5000],\n","        [0.5000, 0.5000]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AyoctcOn32rM"},"source":["As you may have noticed, there is something out of place with these type of operations. That is, in our example we used a rank 2 tensor with a scalar (i.e. a rank 0 tensor). According to the definition at the beginning of the section this should not possible.\n","\n","#### Broadcasting\n","\n","However, this is possible due to broadcasting. In our example the scalar 2 is first broadcasted to a rank 2 tensor with all components equal to 2. Then, the arithmetic operation is carried out as usual. That is, the operations that actually happen are the following:"]},{"cell_type":"code","metadata":{"id":"cPJE1Zio32rN","executionInfo":{"status":"ok","timestamp":1605359157567,"user_tz":-120,"elapsed":1060,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"}},"outputId":"7821f021-6715-4096-eb55-892af23896a1","colab":{"base_uri":"https://localhost:8080/"}},"source":["# The scalar is brocasted to a tensor with the same shape as t1\n","t_broadcast = torch.tensor(np.broadcast_to(2, t1.shape), dtype=torch.float32)\n","print(t_broadcast)\n","\n","# The actual addition between tensors of same shape\n","t = t1 + t_broadcast\n","print(t)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[2., 2.],\n","        [2., 2.]])\n","tensor([[3., 3.],\n","        [3., 3.]])\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n","  \n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"nsXYbcEB32rP"},"source":["Broadcasting also works for different rank tensors. Suppose we have the following two tensors:"]},{"cell_type":"code","metadata":{"id":"0_VGPJiE32rQ","executionInfo":{"status":"ok","timestamp":1605359183630,"user_tz":-120,"elapsed":1320,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"}},"outputId":"03655792-3ad8-4371-8d03-38c431c4d845","colab":{"base_uri":"https://localhost:8080/"}},"source":["t1 = torch.tensor([\n","    [1,2],\n","    [3,4]\n","], dtype=torch.float32)\n","\n","t2 = torch.tensor([\n","    2, 4\n","], dtype=torch.float32)\n","\n","print(t1.shape)\n","print(t2.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([2, 2])\n","torch.Size([2])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wB5TZBu632rS"},"source":["We can do the element-wise operation in a similar manner as before:"]},{"cell_type":"code","metadata":{"id":"3Vyd9oPh32rT","executionInfo":{"status":"error","timestamp":1605432091452,"user_tz":-120,"elapsed":2319,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"}},"outputId":"709ea428-5977-4845-dec0-0edb8e02e633","colab":{"base_uri":"https://localhost:8080/","height":180}},"source":["t = t1 + t2\n","print(t)"],"execution_count":8,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-8617c28a8fb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mt2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 't1' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"bLFZggxe32rW"},"source":["### Comparison operations\n","\n","Comparison operations are also element-wise. For a given comparison operation between two tensors, a new tensor of the same shape is returned with each element containing either a False or True based on the comparison operation. Similarly to arithmetic operations we can use methods or there operator counter part, where complete list can be found [here](https://pytorch.org/docs/stable/tensors.html#torch.Tensor). For example, given the tensor below we can check to see which values are zero as in the following:"]},{"cell_type":"code","metadata":{"id":"mJM6SWP332rX","executionInfo":{"status":"ok","timestamp":1605432099871,"user_tz":-120,"elapsed":599,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"}},"outputId":"35fcc62f-e709-4198-ef27-59da5cebbb9b","colab":{"base_uri":"https://localhost:8080/"}},"source":["t = torch.tensor([\n","    [0,5,0],\n","    [6,0,7],\n","    [0,8,0]\n","], dtype=torch.float32)\n","\n","\n","result = t.eq(0) # equal to method\n","print(result)\n","\n","result = t==0 # equal to operator\n","print(result)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["tensor([[ True, False,  True],\n","        [False,  True, False],\n","        [ True, False,  True]])\n","tensor([[ True, False,  True],\n","        [False,  True, False],\n","        [ True, False,  True]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"waANp_8s32rZ"},"source":["Of course, all other comparison operation are supported, i.e. `.gt()`, `.lt()`, `.le()`, `.ge()` for greater, less, less than or equal and greater than equal, respectively. Try out some of these operation in the TODO's below."]},{"cell_type":"code","metadata":{"id":"mHeZHrYk32ra","executionInfo":{"status":"ok","timestamp":1605359413269,"user_tz":-120,"elapsed":788,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"}},"outputId":"e9b50933-c284-4558-ff7f-ce14385fa288","colab":{"base_uri":"https://localhost:8080/"}},"source":["# TODO 4.6. Test t for values greater than to 6 and print the result\n","result = t.gt(6)\n","print(result)\n","\n","# TODO 4.7. Use the operator instead of t.le(6) and print the result\n","result = t <= 6\n","print(result)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[False, False, False],\n","        [False, False,  True],\n","        [False,  True, False]])\n","tensor([[ True,  True,  True],\n","        [ True,  True, False],\n","        [ True, False,  True]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ta_xQwri32rd"},"source":["### Function operations\n","\n","Function operations are applied to each of the data entries in a tensor, e.g. `.neg()`, `.abs()`, `.sqrt()` and so on. \n","\n","**Exercise 5**\n","\n","Compute some of these operations on the previous tensor `t` in the TODO's below."]},{"cell_type":"code","metadata":{"id":"EOkG2szE32re","executionInfo":{"status":"ok","timestamp":1605432103163,"user_tz":-120,"elapsed":634,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"}},"outputId":"ce783e3b-0b37-4a2b-ec88-79ad5bc8eb36","colab":{"base_uri":"https://localhost:8080/"}},"source":["# TODO 5.1. Compute the 2's complement of t and print the result\n","#result = t.neg()\n","result = t.to(torch.int32).bitwise_not().to(torch.float32) + 1\n","print(result)\n","\n","# TODO 5.2. Compute the absolut value of the previous result and print the result\n","result = result.abs()\n","print(result)\n","\n","# TODO 5.3. Compute the cosine of t and print the result\n","result = t.cos()\n","print(result)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["tensor([[ 0., -5.,  0.],\n","        [-6.,  0., -7.],\n","        [ 0., -8.,  0.]])\n","tensor([[0., 5., 0.],\n","        [6., 0., 7.],\n","        [0., 8., 0.]])\n","tensor([[ 1.0000,  0.2837,  1.0000],\n","        [ 0.9602,  1.0000,  0.7539],\n","        [ 1.0000, -0.1455,  1.0000]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DemU_9bB32rj"},"source":["### Tensor Reduction\n","\n","So far in this course, we've learned that tensors are a generalization of data representation that we use in to implement Machine Learning models. Specifically in regards to ML models, tensors are the data structures which help us manipulate our data and are supported widely by a number of frameworks, including PyTorch. For this reason, tensors are super important, but ultimately, what we are doing with the operations we've been learning about in this course is managing the data elements contained with our tensors. \n","\n","**Reshaping** operations gave us the ability to position our elements along particular axes, while **Element-wise** operations allow us to perform operations on elements between two tensors. **Reduction** operations, on the other hand, allow us to perform operations on elements within a single tensor.\n","\n","\n","**Definition:** *A reduction operation on a tensor is an operation that reduces the number of elements contained within the tensor.*\n","\n","We'll focus mainly on the frequently used [`.argmax()`](https://pytorch.org/docs/stable/tensors.html?highlight=argmax#torch.Tensor.argmax) method to illustrate the reduction operation. Suppose we the following 3x3 rank-2 tensor:"]},{"cell_type":"code","metadata":{"id":"L9497MR232rk","executionInfo":{"status":"ok","timestamp":1605371563653,"user_tz":-120,"elapsed":811,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"}},"outputId":"b9d6c156-2db8-4a65-d78e-9ba5c978c1fb","colab":{"base_uri":"https://localhost:8080/"}},"source":["t = torch.tensor([\n","    [0,1,0],\n","    [2,0,2],\n","    [0,3,0]\n","], dtype=torch.float32)\n","print('t shape: ', t.shape)\n","print('# elements: ', t.numel())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["t shape:  torch.Size([3, 3])\n","# elements:  9\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UulIDjzw32rn"},"source":["**Exercise 6**\n","\n","As a first reduction operation we can use the summation method `.sum()` on a tensor."]},{"cell_type":"code","metadata":{"id":"nOjJivuv32ro","executionInfo":{"status":"ok","timestamp":1605371669695,"user_tz":-120,"elapsed":699,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"}},"outputId":"646fa798-12fd-4129-c1e0-cb41353fff1a","colab":{"base_uri":"https://localhost:8080/"}},"source":["# TODO 6.1. Compute the sum of elements within a tensor\n","sum = t.sum()\n","# TODO 6.2. Print the sum result and its shape\n","print(sum)\n","print(sum.shape)\n","# TODO 6.3. Print result number of elements\n","print(sum.numel())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor(8.)\n","torch.Size([])\n","1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5mgG7XaG32rr"},"source":["As you can see the result of this call is a scalar valued tensor with an element who's value is 8. Meaning that we have reduced our tensor to a rank 0 tensor whose number of elements is 1."]},{"cell_type":"markdown","metadata":{"id":"6vMB1gG232rr"},"source":["#### Common tensor reduction operations\n","\n","Common reduction operations include summing, computing the product, the mean and the standard devision, i.e. `.sum()`, `.prod()`, `.mean()` and `.std()`. We already have seen `.sum()`. Try the other operation on the previous tensor and print the results in the TODO's below"]},{"cell_type":"code","metadata":{"id":"UQ3GkMeE32rs","executionInfo":{"status":"ok","timestamp":1605371965017,"user_tz":-120,"elapsed":621,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"}},"outputId":"555d2e37-0384-4fb5-9f06-fe945a096162","colab":{"base_uri":"https://localhost:8080/"}},"source":["# TODO 6.4. Compute the product of the t tensor elements\n","print(t.prod())\n","\n","# TODO 6.5. Compute the mean of the t tensor elements\n","print(t.mean())\n","\n","# TODO 6.6. Compute the standard devision of the t tensor elements\n","print(t.std())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor(0.)\n","tensor(0.8889)\n","tensor(1.1667)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MkmesT9E32rw"},"source":["All of these tensor methods reduce the tensor to a single element scalar valued tensor by operating on all the tensor's elements. That is, reduction operations in general allow us to compute aggregate (total) values across data structures. In our case, our structures are tensors.\n","\n","**Question:** *Do reduction operations always reduce a tensor to a single element?*\n","\n","**Answer:** Certainly **NO!** \n","\n","In fact, we often reduce specific axes at a time. This process is important. It's just like we saw with reshaping when we aimed to flatten tensors within a batch while still maintaining the batch axis.\n","\n","#### Reducing tensors by axes\n","\n","To reduce a tensor with respect to a specific axis, we use the same methods, and we just pass a value for the dimension `dim=` parameter indicating the axis on which the operation should occur. Suppose we're dealing with the tensor below."]},{"cell_type":"code","metadata":{"id":"w3wKKKVi32rw"},"source":["t = torch.tensor([\n","    [1,1,1,1],\n","    [2,2,2,2],\n","    [3,3,3,3]\n","], dtype=torch.float32)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EbUFXlXV32rz"},"source":["This is a 3 x 4 rank-2 tensor having different lengths for the two axes. Now let's consider the `.sum()` method again. Only, this time, we will specify a dimension to reduce. We have two axes so we'll do both to see what result they yield."]},{"cell_type":"code","metadata":{"id":"XCaL1J9E32r0","executionInfo":{"status":"ok","timestamp":1605372049753,"user_tz":-120,"elapsed":709,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"}},"outputId":"c1c1e69b-be77-42f4-b26d-4e123f675455","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Summing on the first dimension\n","t.sum(dim=0)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([6., 6., 6., 6.])"]},"metadata":{"tags":[]},"execution_count":87}]},{"cell_type":"code","metadata":{"id":"8wZEwfTq32r2","executionInfo":{"status":"ok","timestamp":1605372077303,"user_tz":-120,"elapsed":595,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"}},"outputId":"9cb44171-f290-4958-cdfe-6aa05ae776da","colab":{"base_uri":"https://localhost:8080/"}},"source":["# TODO 6.7. Sum on the second dimension\n","t.sum(dim=1)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 4.,  8., 12.])"]},"metadata":{"tags":[]},"execution_count":88}]},{"cell_type":"markdown","metadata":{"id":"kAJAMW3t32r4"},"source":["As you can see, the summing operation on the first dimension produces a resulting tensor of 4 elements that represents the addition of rows, i.e. addition along the the first axis. Similarly, for the second summing operation the addition proceeds along the columns of the input tensor. We usually refer to this type of operation as  proceeding along some axis of the input tensor. "]},{"cell_type":"markdown","metadata":{"id":"Q43cDpGD32r5"},"source":["#### Argmax  tensor reduction operation example\n","\n","`argmax()` is a mathematical function that tells us which argument, when supplied to a function as input, results in the function's max output value. Hence, in software terms, `argmax()` returns the index location of the maximum value inside a tensor.\n","\n","When we call the argmax() method on a tensor, the tensor is reduced to a new tensor that contains an index value indicating where the max value is inside the tensor. Given the tensor below."]},{"cell_type":"code","metadata":{"id":"f3nS5zBk32r6"},"source":["t = torch.tensor([\n","    [1,0,0,2],\n","    [0,3,3,0],\n","    [4,0,0,5]\n","], dtype=torch.float32)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fwxvhP8x32r8"},"source":["What's it max value? Complete the TODO below."]},{"cell_type":"code","metadata":{"id":"0sEJt9Xe32r9","executionInfo":{"status":"ok","timestamp":1605372266035,"user_tz":-120,"elapsed":706,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"}},"outputId":"fdbcc832-e712-4f22-b712-91f5f53a444c","colab":{"base_uri":"https://localhost:8080/"}},"source":["# TODO 6.8. Compute the maximun value of t\n","t.max()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(5.)"]},"metadata":{"tags":[]},"execution_count":95}]},{"cell_type":"markdown","metadata":{"id":"ATEts7gD32r_"},"source":["However, when we use the `argmax()` method, the resulting tensor is?"]},{"cell_type":"code","metadata":{"id":"3H3fHU8532r_","executionInfo":{"status":"ok","timestamp":1605372271284,"user_tz":-120,"elapsed":883,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"}},"outputId":"73ef52d7-f960-45a3-d5b2-13210e2dc2bc","colab":{"base_uri":"https://localhost:8080/"}},"source":["# TODO 6.9. Compute the argmax of tensor t\n","t.argmax()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(11)"]},"metadata":{"tags":[]},"execution_count":96}]},{"cell_type":"markdown","metadata":{"id":"-YjKJrEV32sC"},"source":["This does not return the index pair (2, 3) corresponding to the maximum value. Instead we get a single index which is 11. The reason for this is that without supplying a dimension the result is an index from the flattened tensor. To see this complete the TODO's below."]},{"cell_type":"code","metadata":{"id":"-b_5ZJNd32sD","executionInfo":{"status":"ok","timestamp":1605372369875,"user_tz":-120,"elapsed":750,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"}},"outputId":"cc34829b-11bc-4c0d-8a36-12a142826114","colab":{"base_uri":"https://localhost:8080/"}},"source":["# TODO 6.10. Flatten the tesor t\n","t_flatten = t.flatten()\n","\n","# TODO 6.11. Print the flattened tensor\n","print(t_flatten)\n","\n","# TODO 6.12. Print the maximum of the flatten tensor\n","print(t_flatten.max())\n","\n","# TODO 6.13. Compute the argmax() of the flatten tensor\n","argmax = t_flatten.argmax()\n","\n","# TODO 6.14. Print the result\n","print(argmax)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([1., 0., 0., 2., 0., 3., 3., 0., 4., 0., 0., 5.])\n","tensor(5.)\n","tensor(11)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"usSYUhxe32sH"},"source":["Now if we compute the maximum and argmax by supplying different dimension we will get the actual results we were expecting in the first place."]},{"cell_type":"code","metadata":{"id":"6vx3bDse32sH","executionInfo":{"status":"ok","timestamp":1605372871251,"user_tz":-120,"elapsed":645,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"}},"outputId":"11952996-b840-438d-a64b-6018a03323ae","colab":{"base_uri":"https://localhost:8080/"}},"source":["# TODO 6.15. Print the maximum values in each row\n","print(t.max(dim = 1))\n","\n","# TODO 6.16. Print the argmax values in each row\n","print(t.argmax(dim = 1))\n","\n","# TODO 6.17. Print the maximum values in each column\n","print(t.max(dim = 0))\n","\n","# TODO 6.18. Print the argmax values in each column\n","print(t.argmax(dim = 0))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.return_types.max(\n","values=tensor([2., 3., 5.]),\n","indices=tensor([3, 1, 3]))\n","tensor([3, 1, 3])\n","torch.return_types.max(\n","values=tensor([4., 3., 3., 5.]),\n","indices=tensor([2, 1, 1, 2]))\n","tensor([2, 1, 1, 2])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ipuPbiKU32sK"},"source":["Notice how the call to the `max()` method returns two tensors. The first tensor contains the max values and the second tensor contains the index locations for the max values. The index location is what `argmax()` gives us.\n","\n","For the first axis, the max values are, 4, 3, 3, and 5. These values are determined by taking the element-wise maximum across each array running across the first axis.\n","\n","For each of these maximum values, the `argmax()` method tells us which element along the first axis where the value lives.\n","\n","The 4 lives at index two of the first axis.\n","The first 3 lives at index one of the first axis.\n","The second 3 lives at index one of the first axis.\n","The 5 lives at index two of the first axis.\n","For the second axis, the max values are 2, 3, and 5. These values are determined by taking the maximum inside each array of the second axis. We have three groups of four, which gives us 3 maximum values.\n","\n","The argmax values here, tell the index inside each respective array where the max value lives.\n","\n","In practice, we often use the argmax() function on a network’s output prediction tensor, to determine which category has the highest prediction value."]},{"cell_type":"markdown","metadata":{"id":"fpLbHMsz32sK"},"source":["### Accessing elements inside tensors\n","\n","The last common type of operation that we need for tensors is the ability to access data from within the tensor. Suppose we have the following tensor:"]},{"cell_type":"code","metadata":{"id":"-gyYNNdf32sM"},"source":["import torch\n","\n","t = torch.tensor([\n","    [1,2,3],\n","    [4,5,6],\n","    [7,8,9]\n","], dtype=torch.float32)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oDNjz9oq32sO"},"source":["**Exercise 7**\n","\n","Compute the mean of the entire tensor and print the result."]},{"cell_type":"code","metadata":{"id":"a8BQT3Q-32sO"},"source":["# TODO 7.1. Compute the mean of elements within the tensor\n","mean = t.mean()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lMfX0Fx732sQ"},"source":["As expected we get an rank 0 tensor that contains the mean. To actually get the value as a number, we use the [`.item()`](https://pytorch.org/docs/stable/tensors.html?highlight=item#torch.Tensor.item) tensor method which works for scalar valued tensors."]},{"cell_type":"code","metadata":{"id":"1J2mzoG_32sR","executionInfo":{"status":"ok","timestamp":1605373038253,"user_tz":-120,"elapsed":1205,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"}},"outputId":"9f8a9fe7-af7e-48fa-c1bd-c97058cf0a03","colab":{"base_uri":"https://localhost:8080/"}},"source":["# TODO 7.2. Call item() on the mean of the tensor\n","mean.item()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5.0"]},"metadata":{"tags":[]},"execution_count":112}]},{"cell_type":"markdown","metadata":{"id":"14Z7iDAS32sT"},"source":["For multiple values we have to use some conversion methods such as `.tolist()`  to convert the result to a Python list or `.numpy()` to convert the result to a Numpy array. "]},{"cell_type":"code","metadata":{"id":"z9NZrLR_32sT","executionInfo":{"status":"ok","timestamp":1605373253815,"user_tz":-120,"elapsed":845,"user":{"displayName":"Ill Andy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiEM66iTkGJV9MAWifLkRXvucMYE6dpHdLn_DHy=s64","userId":"13087998379211610683"}},"outputId":"a6aa6b33-fb0b-4529-eff0-c56767598d9e","colab":{"base_uri":"https://localhost:8080/"}},"source":["# TOOD 7.3. Compute the mean along the first axis\n","result = t.mean(dim = 0)\n","\n","# TOOD 7.4. Print the result\n","print(result)\n","\n","\n","# TOOD 7.5. Convert the result to a Python list\n","result_list = result.tolist()\n","\n","# TOOD 7.6. Print the result\n","print(result_list)\n","\n","# TOOD 7.7. Convert the result to a Numpy array\n","result_np = result.numpy()\n","\n","# TOOD 7.8. Print the result\n","print(result_np)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([4., 5., 6.])\n","[4.0, 5.0, 6.0]\n","[4. 5. 6.]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1Z1if9YR32sV"},"source":["**Remember:** When we compute the mean across the first axis, multiple values are returned, and we can access the numeric values by transforming the output tensor into a Python list or a NumPy array.\n","\n","\n","With NumPy ndarray objects, we have a pretty robust set of operations for indexing and slicing, and PyTorch tensor objects support most of these operations as well. Use this a resource for [advanced indexing and slicing](https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html)."]},{"cell_type":"code","metadata":{"id":"rP-I2gr532sV"},"source":[""],"execution_count":null,"outputs":[]}]}